{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>P</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/1/1921</td>\n",
       "      <td>50.711506</td>\n",
       "      <td>9.680717</td>\n",
       "      <td>1.173617</td>\n",
       "      <td>81.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/1/1921</td>\n",
       "      <td>42.727805</td>\n",
       "      <td>6.427311</td>\n",
       "      <td>2.666764</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/1/1921</td>\n",
       "      <td>36.407265</td>\n",
       "      <td>3.427978</td>\n",
       "      <td>10.194451</td>\n",
       "      <td>194.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/1922</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.775463</td>\n",
       "      <td>2.969249</td>\n",
       "      <td>192.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/1/1922</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.077533</td>\n",
       "      <td>14.370800</td>\n",
       "      <td>422.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>5/1/2021</td>\n",
       "      <td>54.776440</td>\n",
       "      <td>11.438191</td>\n",
       "      <td>0.265424</td>\n",
       "      <td>136.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>6/1/2021</td>\n",
       "      <td>66.966054</td>\n",
       "      <td>19.161114</td>\n",
       "      <td>0.136572</td>\n",
       "      <td>88.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>7/1/2021</td>\n",
       "      <td>73.537018</td>\n",
       "      <td>24.017705</td>\n",
       "      <td>0.086182</td>\n",
       "      <td>70.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>8/1/2021</td>\n",
       "      <td>70.023603</td>\n",
       "      <td>21.625689</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>56.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>9/1/2021</td>\n",
       "      <td>64.739571</td>\n",
       "      <td>18.231172</td>\n",
       "      <td>0.582271</td>\n",
       "      <td>56.281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date          T          V          P        F\n",
       "0     10/1/1921  50.711506   9.680717   1.173617   81.200\n",
       "1     11/1/1921  42.727805   6.427311   2.666764  100.000\n",
       "2     12/1/1921  36.407265   3.427978  10.194451  194.000\n",
       "3      1/1/1922  32.000000   2.775463   2.969249  192.000\n",
       "4      2/1/1922  32.000000   2.077533  14.370800  422.000\n",
       "...         ...        ...        ...        ...      ...\n",
       "1195   5/1/2021  54.776440  11.438191   0.265424  136.858\n",
       "1196   6/1/2021  66.966054  19.161114   0.136572   88.619\n",
       "1197   7/1/2021  73.537018  24.017705   0.086182   70.112\n",
       "1198   8/1/2021  70.023603  21.625689   0.024613   56.391\n",
       "1199   9/1/2021  64.739571  18.231172   0.582271   56.281\n",
       "\n",
       "[1200 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "columns_to_analyze = ['Date','T', 'V', 'P', 'F']\n",
    "df = df[columns_to_analyze]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\sparse\\__init__.py:308\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# For backward compatibility with v0.19.\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csgraph\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    312\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    313\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:185\u001b[0m\n\u001b[0;32m    157\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestructuredtext en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconnected_components\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    160\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaplacian\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_path\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsgraph_to_masked\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegativeCycleError\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_laplacian\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m laplacian\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shortest_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    187\u001b[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson,\n\u001b[0;32m    188\u001b[0m     NegativeCycleError\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traversal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    191\u001b[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001b[0;32m    192\u001b[0m     depth_first_tree, connected_components\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_pydata_sparse_to_scipy, is_pydata_spmatrix\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Graph laplacian\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:129\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m==================================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dsolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterative Solvers for Sparse Linear Systems\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#from info import __doc__\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminres\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minres\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlgmres\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lgmres\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_system\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _NoValue, _deprecate_positional_args\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicgstab\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcgs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmres\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqmr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\linalg\\__init__.py:203\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m====================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mLinear algebra (:mod:`scipy.linalg`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_misc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cythonized_array_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_basic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\linalg\\_misc.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinAlgError\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinAlgError\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinAlgWarning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\scipy\\linalg\\blas.py:213\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_np\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fblas\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cblas\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.activations import relu\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initial random seed setting\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Smoothness penalty coefficient\n",
    "beta = 0.1   # Monthly weight coefficient\n",
    "epochs = 5000  # Training epochs (increased from 2000 to 5000)\n",
    "input_window = 12\n",
    "output_window = 9\n",
    "n_features = 4  # PC1 to PC4\n",
    "batch_size = 32\n",
    "\n",
    "# Define LSTM architectures\n",
    "lstm_architectures = [\n",
    "    [64, 32, 16],  # Original architecture\n",
    "    [32, 16, 8],   # Simpler 3-layer\n",
    "    [16, 8, 4],    # Simpler 3-layer\n",
    "    [8, 4, 2],     # Simpler 3-layer\n",
    "    [32, 16],      # 2-layer\n",
    "    [16, 8],       # 2-layer\n",
    "    [8, 4],        # 2-layer\n",
    "    [4, 2]         # 2-layer\n",
    "]\n",
    "\n",
    "# Custom loss function\n",
    "def create_custom_flow_loss(alpha, beta):\n",
    "    def custom_flow_loss(y_true, y_pred):\n",
    "        month_weights = tf.constant([2, 2, 2, 1.5, 1.5, 1, 1, 1, 1], dtype=tf.float32)\n",
    "        mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "        \n",
    "        # Smoothness penalty\n",
    "        smoothness_penalty = tf.reduce_mean(tf.square(y_pred[:, 1:] - y_pred[:, :-1]))\n",
    "        \n",
    "        # Monthly weighted loss\n",
    "        monthly_weight_loss = tf.reduce_mean(month_weights * tf.square(y_true - y_pred))\n",
    "        \n",
    "        return mse_loss + alpha * smoothness_penalty + beta * monthly_weight_loss\n",
    "    return custom_flow_loss\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=4)\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        features = df[['T', 'V', 'P', 'F']].values\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        pca_features = self.pca.fit_transform(features_scaled)\n",
    "        return pca_features\n",
    "    \n",
    "    def transform(self, df):\n",
    "        features = df[['T', 'V', 'P', 'F']].values\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        pca_features = self.pca.transform(features_scaled)\n",
    "        return pca_features\n",
    "    \n",
    "    def save(self, filename):\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca\n",
    "        }\n",
    "        joblib.dump(preprocessor_dict, filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        preprocessor = cls()\n",
    "        loaded_dict = joblib.load(filename)\n",
    "        preprocessor.scaler = loaded_dict['scaler']\n",
    "        preprocessor.pca = loaded_dict['pca']\n",
    "        return preprocessor\n",
    "\n",
    "def prepare_sequences(features, target):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - input_window - output_window + 1):\n",
    "        X.append(features[i:(i + input_window)])\n",
    "        y.append(target[i + input_window:i + input_window + output_window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(architecture):\n",
    "    # Ensure clean state for model creation\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(architecture[0], activation='relu', return_sequences=True if len(architecture) > 1 else False,\n",
    "                  input_shape=(input_window, n_features)))\n",
    "    \n",
    "    # Middle LSTM layers\n",
    "    for i in range(1, len(architecture) - 1):\n",
    "        model.add(LSTM(architecture[i], activation='relu', return_sequences=True))\n",
    "    \n",
    "    # Last LSTM layer\n",
    "    if len(architecture) > 1:\n",
    "        model.add(LSTM(architecture[-1], activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(output_window, activation='relu'))\n",
    "    \n",
    "    custom_loss = create_custom_flow_loss(alpha, beta)\n",
    "    model.compile(optimizer='adam', loss=custom_loss)\n",
    "    return model\n",
    "\n",
    "def plot_combined_losses(all_histories, save_path='combined_losses_1943_2021.png'):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create a color map for different architectures\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(all_histories)))\n",
    "    \n",
    "    for (model_name, history), color in zip(all_histories.items(), colors):\n",
    "        # Extract architecture details directly from the model name\n",
    "        # Format: lstm_h{num_layers}_{neurons}_a{alpha}_b{beta}_1943_2021\n",
    "        parts = model_name.split('_')\n",
    "        \n",
    "        # Get number of layers (h2 or h3)\n",
    "        num_layers = parts[1]  # This will be 'h2' or 'h3'\n",
    "        \n",
    "        # Get neuron configuration - extract the numbers between the layer indicator and alpha\n",
    "        neuron_parts = []\n",
    "        i = 2\n",
    "        while i < len(parts) and not parts[i].startswith('a'):\n",
    "            neuron_parts.append(parts[i])\n",
    "            i += 1\n",
    "            \n",
    "        neurons = '_'.join(neuron_parts)\n",
    "        label = f\"{num_layers}_{neurons}\"\n",
    "        \n",
    "        # Plot with custom formatting\n",
    "        plt.plot(history['loss'], label=label, color=color, alpha=0.8)\n",
    "    \n",
    "    plt.title('Training Loss Comparison Across Different Architectures (1943-2021)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def get_model_name(architecture, alpha, beta):\n",
    "    # Create name based on number of hidden layers and neurons\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"lstm_{arch_str}_a{alpha}_b{beta}_1943_2021\"\n",
    "\n",
    "def train_models(df):\n",
    "    # Initialize and fit preprocessor (only once)\n",
    "    preprocessor = DataPreprocessor()\n",
    "    pca_features = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor.save('preprocessor_1943_2021.joblib')\n",
    "    \n",
    "    # Prepare sequences (only once)\n",
    "    X, y = prepare_sequences(pca_features, df['F'].values)\n",
    "    \n",
    "    # Dictionary to store all training histories\n",
    "    all_histories = {}\n",
    "    \n",
    "    # Train models with different architectures\n",
    "    for architecture in lstm_architectures:\n",
    "        # Clear everything from previous iteration\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Reset random seeds for complete reproducibility\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "        \n",
    "        # Get model name\n",
    "        model_name = get_model_name(architecture, alpha, beta)\n",
    "        print(f\"\\nTraining model: {model_name}\")\n",
    "        print(f\"Architecture: {architecture}\")\n",
    "        \n",
    "        # Build and train model\n",
    "        model = build_model(architecture)\n",
    "        print(model.summary())\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(X, y,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          verbose=0)\n",
    "        \n",
    "        # Save model\n",
    "        model.save(f'{model_name}.keras')\n",
    "        \n",
    "        # Store training history\n",
    "        all_histories[model_name] = history.history\n",
    "        \n",
    "        # Print current model's final loss\n",
    "        final_loss = history.history['loss'][-1]\n",
    "        print(f\"\\nFinal loss for {model_name}: {final_loss:.4f}\")\n",
    "    \n",
    "    # Plot combined losses\n",
    "    plot_combined_losses(all_histories)\n",
    "    \n",
    "    # Print comparative summary\n",
    "    print(\"\\nFinal losses for all models:\")\n",
    "    for model_name, history in all_histories.items():\n",
    "        print(f\"{model_name}: {history['loss'][-1]:.4f}\")\n",
    "    \n",
    "    return all_histories, preprocessor\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        \n",
    "        df_full = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        \n",
    "        # Convert date column to datetime if it's not already\n",
    "        if 'Date' in df_full.columns:\n",
    "            df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
    "            \n",
    "            # Filter data from 1/1/1943 to the end of the dataset\n",
    "            start_date = pd.to_datetime('1943-01-01')\n",
    "            \n",
    "            # Use the entire dataset from 1943 onwards\n",
    "            df_filtered = df_full[df_full['Date'] >= start_date]\n",
    "            \n",
    "            print(f\"Full dataset size: {len(df_full)}\")\n",
    "            print(f\"Filtered dataset size (1943-2021): {len(df_filtered)}\")\n",
    "            \n",
    "            # Train all models using the filtered dataset\n",
    "            print(\"Starting training process on filtered data (1943-2021)...\")\n",
    "            all_histories, preprocessor = train_models(df_filtered)\n",
    "            print(\"\\nTraining completed successfully!\")\n",
    "            \n",
    "            # Save final plot\n",
    "            print(\"Combined loss plot saved as 'combined_losses_1943_2021.png'\")\n",
    "        else:\n",
    "            print(\"Error: Dataset does not contain a 'Date' column. Please adjust the code to match your date column name.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Starting forecast generation for all models...\n",
      "Found 6 model files:\n",
      " - lstm_h2_16_8_a0.1_b0.1_1943_2021.keras\n",
      " - lstm_h2_32_16_a0.1_b0.1_1943_2021.keras\n",
      " - lstm_h3_16_8_4_a0.1_b0.1_1943_2021.keras\n",
      " - lstm_h3_32_16_8_a0.1_b0.1_1943_2021.keras\n",
      " - lstm_h3_64_32_16_a0.1_b0.1_1943_2021.keras\n",
      " - lstm_h3_8_4_2_a0.1_b0.1_1943_2021.keras\n",
      "Found 1 preprocessor files:\n",
      " - preprocessor_1943_2021.joblib\n",
      "Using preprocessor file: preprocessor_1943_2021.joblib\n",
      "\n",
      "Looking for model: lstm_h3_64_32_16_a0.1_b0.1_1943_2021\n",
      "Found matching model: lstm_h3_64_32_16_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded model from lstm_h3_64_32_16_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded preprocessor from preprocessor_1943_2021.joblib\n",
      "Saved forecasts to forecasts_lstm_h3_64_32_16_a0.1_b0.1_1943_2021.csv\n",
      "Shape of forecast dataframe: (99, 10)\n",
      "\n",
      "First few forecasts:\n",
      "   Year        F_Jan       F_Feb  F_Mar       F_Apr       F_May       F_Jun  \\\n",
      "0  1923  1227.728394  445.914825    0.0  774.856140  511.218536  291.468201   \n",
      "1  1924   251.990067  354.200470    0.0  374.975403  521.676147  371.499603   \n",
      "2  1925   511.644470  145.737701    0.0  435.516296  504.825470  213.114639   \n",
      "3  1926   285.807587  107.387268    0.0  382.762939  475.266327  251.782440   \n",
      "4  1927   327.918518  695.992126    0.0  748.983582  662.483215  366.158691   \n",
      "\n",
      "        F_Jul       F_Aug       F_Sep  \n",
      "0  155.921982  108.705093   93.702187  \n",
      "1  198.026306   99.227722   73.909363  \n",
      "2  114.146706   53.419613   60.207245  \n",
      "3  118.974442   72.577461   46.373341  \n",
      "4  191.595291  114.218422  105.772041  \n",
      "\n",
      "Looking for model: lstm_h3_32_16_8_a0.1_b0.1_1943_2021\n",
      "Found matching model: lstm_h3_32_16_8_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded model from lstm_h3_32_16_8_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded preprocessor from preprocessor_1943_2021.joblib\n",
      "Saved forecasts to forecasts_lstm_h3_32_16_8_a0.1_b0.1_1943_2021.csv\n",
      "Shape of forecast dataframe: (99, 10)\n",
      "\n",
      "First few forecasts:\n",
      "   Year        F_Jan        F_Feb        F_Mar        F_Apr       F_May  \\\n",
      "0  1923  1570.258789  1360.770996  1019.464661  1130.775635  929.001465   \n",
      "1  1924   244.177963   537.248596   738.356567   698.768555  514.314026   \n",
      "2  1925   193.573212   163.632034   282.496887   285.819641  215.074051   \n",
      "3  1926    97.547409   263.643829   386.492859   445.495209  363.843445   \n",
      "4  1927   999.705750   778.627930   931.906738   997.304260  914.094666   \n",
      "\n",
      "        F_Jun       F_Jul       F_Aug       F_Sep  \n",
      "0  581.364258   80.303726  113.539619  173.370850  \n",
      "1  348.047058  155.928772   98.564285   88.505753  \n",
      "2  101.094490   53.211624   56.721043   56.328442  \n",
      "3  291.397919  131.424973   69.826981   68.789101  \n",
      "4  609.468994  343.905670  121.172012  115.749855  \n",
      "\n",
      "Looking for model: lstm_h3_16_8_4_a0.1_b0.1_1943_2021\n",
      "Found matching model: lstm_h3_16_8_4_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded model from lstm_h3_16_8_4_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded preprocessor from preprocessor_1943_2021.joblib\n",
      "Saved forecasts to forecasts_lstm_h3_16_8_4_a0.1_b0.1_1943_2021.csv\n",
      "Shape of forecast dataframe: (99, 10)\n",
      "\n",
      "First few forecasts:\n",
      "   Year       F_Jan        F_Feb        F_Mar        F_Apr  F_May       F_Jun  \\\n",
      "0  1923  805.456055   956.540833   989.193970   829.531921    0.0  247.442642   \n",
      "1  1924  283.203918   331.152039   350.592896   315.735077    0.0  163.784958   \n",
      "2  1925  765.042419   803.512268   754.708557   587.374695    0.0   58.779259   \n",
      "3  1926  235.543930   324.188782   381.279053   360.450653    0.0  216.769318   \n",
      "4  1927  935.699646  1342.480469  1555.713623  1394.616699    0.0  629.217773   \n",
      "\n",
      "   F_Jul       F_Aug       F_Sep  \n",
      "0    0.0   74.152374   24.111883  \n",
      "1    0.0  119.017715  108.862892  \n",
      "2    0.0   63.275322   77.054298  \n",
      "3    0.0   68.028725   24.724045  \n",
      "4    0.0    2.553150    0.000000  \n",
      "\n",
      "Looking for model: lstm_h3_8_4_2_a0.1_b0.1_1943_2021\n",
      "Found matching model: lstm_h3_8_4_2_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded model from lstm_h3_8_4_2_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded preprocessor from preprocessor_1943_2021.joblib\n",
      "Saved forecasts to forecasts_lstm_h3_8_4_2_a0.1_b0.1_1943_2021.csv\n",
      "Shape of forecast dataframe: (99, 10)\n",
      "\n",
      "First few forecasts:\n",
      "   Year        F_Jan        F_Feb        F_Mar       F_Apr       F_May  \\\n",
      "0  1923   990.235413  1072.126709  1002.534424  868.710022  707.820190   \n",
      "1  1924   391.536621   506.931976   551.308411  530.009094  473.752350   \n",
      "2  1925   290.252716   343.197357   353.186523  326.132080  282.634888   \n",
      "3  1926   144.002792   345.321869   493.600830  545.054871  537.287170   \n",
      "4  1927  1259.215576  1282.884277  1128.077637  927.987366  716.956970   \n",
      "\n",
      "        F_Jun       F_Jul       F_Aug  F_Sep  \n",
      "0  500.190613  193.937363    0.000000    0.0  \n",
      "1  391.219910  250.202515  126.365860    0.0  \n",
      "2  236.268448  155.944168   90.922668    0.0  \n",
      "3  490.603149  383.263702  255.554855    0.0  \n",
      "4  462.632324  102.891319    0.000000    0.0  \n",
      "\n",
      "Looking for model: lstm_h2_32_16_a0.1_b0.1_1943_2021\n",
      "Found matching model: lstm_h2_32_16_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded model from lstm_h2_32_16_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded preprocessor from preprocessor_1943_2021.joblib\n",
      "Saved forecasts to forecasts_lstm_h2_32_16_a0.1_b0.1_1943_2021.csv\n",
      "Shape of forecast dataframe: (99, 10)\n",
      "\n",
      "First few forecasts:\n",
      "   Year       F_Jan       F_Feb       F_Mar       F_Apr       F_May  \\\n",
      "0  1923  418.426086  961.611816  619.609131  752.239136  566.742737   \n",
      "1  1924  564.039368  634.315613  431.843597  423.748993  475.038910   \n",
      "2  1925  459.904480  239.284637  182.858963  403.982391  246.089996   \n",
      "3  1926  243.895920  495.930450  582.858154  589.828979  502.575500   \n",
      "4  1927  332.575012  865.172607  975.119263  844.820435  772.040710   \n",
      "\n",
      "        F_Jun       F_Jul       F_Aug       F_Sep  \n",
      "0  360.176575  229.641769  130.589142  112.521271  \n",
      "1  290.351501  185.412582  122.389595  125.468979  \n",
      "2  114.029305   34.818420   94.496727   52.525097  \n",
      "3  300.433472  134.462936  117.828079   89.741455  \n",
      "4  457.771698  209.617477   90.911179  103.764389  \n",
      "\n",
      "Looking for model: lstm_h2_16_8_a0.1_b0.1_1943_2021\n",
      "Found matching model: lstm_h2_16_8_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded model from lstm_h2_16_8_a0.1_b0.1_1943_2021.keras\n",
      "Successfully loaded preprocessor from preprocessor_1943_2021.joblib\n",
      "Saved forecasts to forecasts_lstm_h2_16_8_a0.1_b0.1_1943_2021.csv\n",
      "Shape of forecast dataframe: (99, 10)\n",
      "\n",
      "First few forecasts:\n",
      "   Year       F_Jan        F_Feb        F_Mar       F_Apr       F_May  \\\n",
      "0  1923  294.305054   341.426575   438.800201  454.119568  434.020844   \n",
      "1  1924  191.481705   193.263153   282.021973  388.051025  381.329926   \n",
      "2  1925  589.166626   619.509949   705.197144  662.324707  424.323914   \n",
      "3  1926  140.552246   199.979568   207.087982  214.118576  233.800446   \n",
      "4  1927  751.399841  1112.625488  1106.731567  775.569153  425.069153   \n",
      "\n",
      "        F_Jun       F_Jul       F_Aug       F_Sep  \n",
      "0  269.193115  171.680435  125.434906  103.103798  \n",
      "1  232.203827   97.285019   53.781311   57.429527  \n",
      "2  207.173889  144.735001   99.564865   54.136185  \n",
      "3  177.655746   68.810379   34.537323   65.319778  \n",
      "4  220.087769   85.316864   81.077332  186.314056  \n",
      "\n",
      "Forecast generation completed! Generated forecasts for 6 models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define hyperparameters\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "# Define LSTM architectures (updated to include [8, 4, 2])\n",
    "lstm_architectures = [\n",
    "    [64, 32, 16],  # Original architecture\n",
    "    [32, 16, 8],   # Simpler 3-layer\n",
    "    [16, 8, 4],    # Simpler 3-layer\n",
    "    [8, 4, 2],     # Simpler 3-layer\n",
    "    [32, 16],      # 2-layer\n",
    "    [16, 8],       # 2-layer\n",
    "]\n",
    "\n",
    "def find_model_files():\n",
    "    \"\"\"Find available model files in the current directory and subdirectories\"\"\"\n",
    "    # Look for .keras files\n",
    "    keras_files = glob(\"*.keras\") + glob(\"*/*.keras\")\n",
    "    # Also look for .h5 files (older TensorFlow format)\n",
    "    h5_files = glob(\"*.h5\") + glob(\"*/*.h5\")\n",
    "    \n",
    "    all_files = keras_files + h5_files\n",
    "    print(f\"Found {len(all_files)} model files:\")\n",
    "    for file in all_files:\n",
    "        print(f\" - {file}\")\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def get_model_name(architecture, alpha, beta):\n",
    "    \"\"\"Generate consistent model name\"\"\"\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"lstm_{arch_str}_a{alpha}_b{beta}_1943_2021\"  # Updated to match training naming\n",
    "\n",
    "def load_saved_components(model_path, preprocessor_path):\n",
    "    \"\"\"Load the saved model and preprocessor\"\"\"\n",
    "    try:\n",
    "        model = load_model(model_path, compile=False)  # We'll recompile with custom loss\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        preprocessor = joblib.load(preprocessor_path)\n",
    "        print(f\"Successfully loaded preprocessor from {preprocessor_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading preprocessor from {preprocessor_path}: {str(e)}\")\n",
    "    \n",
    "    return model, preprocessor['scaler'], preprocessor['pca']\n",
    "\n",
    "def prepare_input_sequence(df, scaler, pca, end_date, input_window=12):\n",
    "    \"\"\"Prepare input sequence for a specific end date (December)\"\"\"\n",
    "    # Get the last input_window months of data up to December\n",
    "    mask = df.index <= end_date\n",
    "    recent_data = df.loc[mask].tail(input_window)\n",
    "    \n",
    "    # Transform the data using saved preprocessor components\n",
    "    features = recent_data[['T', 'V', 'P', 'F']].values\n",
    "    features_scaled = scaler.transform(features)\n",
    "    features_pca = pca.transform(features_scaled)\n",
    "    \n",
    "    # Reshape for LSTM input [samples, time steps, features]\n",
    "    return np.expand_dims(features_pca, axis=0)\n",
    "\n",
    "def generate_forecasts(df, model_path, preprocessor_path):\n",
    "    \"\"\"Generate forecasts for Jan-Sep of each year\"\"\"\n",
    "    # Load saved components\n",
    "    model, scaler, pca = load_saved_components(model_path, preprocessor_path)\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    years = []\n",
    "    forecasts = []\n",
    "    \n",
    "    # Convert the index to datetime if it's not already\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Generate forecasts for each December\n",
    "    for year in range(1922, 2021):  # Modified to start from 1922 (for 1923 forecasts)\n",
    "        december = pd.Timestamp(f\"{year}-12-01\")\n",
    "        \n",
    "        # Skip if we don't have enough historical data\n",
    "        if december < df.index[11]:  # Need at least 12 months of history\n",
    "            print(f\"Skipping year {year+1} forecast: insufficient historical data\")\n",
    "            continue\n",
    "            \n",
    "        # Prepare input sequence\n",
    "        X = prepare_input_sequence(df, scaler, pca, december)\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = model.predict(X, verbose=0)[0]  # [0] to get first (only) sample\n",
    "        \n",
    "        # Store results\n",
    "        years.append(year + 1)  # +1 because forecast is for next year\n",
    "        forecasts.append(forecast)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(forecasts, index=years, \n",
    "                            columns=['F_Jan', 'F_Feb', 'F_Mar', 'F_Apr', 'F_May',\n",
    "                                   'F_Jun', 'F_Jul', 'F_Aug', 'F_Sep'])\n",
    "    \n",
    "    # Reset index and rename it to \"Year\"\n",
    "    results_df = results_df.reset_index().rename(columns={'index': 'Year'})\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def find_preprocessor_file():\n",
    "    \"\"\"Find preprocessor file in current directory or subdirectories\"\"\"\n",
    "    preprocessor_files = glob(\"*.joblib\") + glob(\"*/*.joblib\")\n",
    "    \n",
    "    print(f\"Found {len(preprocessor_files)} preprocessor files:\")\n",
    "    for file in preprocessor_files:\n",
    "        print(f\" - {file}\")\n",
    "    \n",
    "    # Try to find the preprocessor file with 1943_2021 first\n",
    "    for file in preprocessor_files:\n",
    "        if \"preprocessor_1943_2021\" in file.lower():\n",
    "            print(f\"Using preprocessor file: {file}\")\n",
    "            return file\n",
    "    \n",
    "    # Try to find any preprocessor file\n",
    "    for file in preprocessor_files:\n",
    "        if \"preprocessor\" in file.lower():\n",
    "            print(f\"Using preprocessor file: {file}\")\n",
    "            return file\n",
    "    \n",
    "    # If no standard preprocessor file found, use the first one\n",
    "    if preprocessor_files:\n",
    "        print(f\"Using preprocessor file: {preprocessor_files[0]}\")\n",
    "        return preprocessor_files[0]\n",
    "    \n",
    "    return 'preprocessor_1943_2021.joblib'  # Updated default path\n",
    "\n",
    "def generate_all_forecasts(df):\n",
    "    \"\"\"Generate forecasts for all architectures\"\"\"\n",
    "    # Find available model files\n",
    "    available_model_files = find_model_files()\n",
    "    \n",
    "    # Find preprocessor file\n",
    "    preprocessor_path = find_preprocessor_file()\n",
    "    \n",
    "    # Dictionary to store all forecast DataFrames\n",
    "    all_forecasts = {}\n",
    "    \n",
    "    # Clear any existing session\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Check if any model files were found\n",
    "    if not available_model_files:\n",
    "        print(\"\\nNo model files found. Please make sure you have trained the models first.\")\n",
    "        print(\"Expected model files should have names like: lstm_h3_64_32_16_a0.1_b0.1_1943_2021.keras\")\n",
    "        print(\"If the models are in a different directory, please copy them to the current directory or update the code to look in the correct location.\")\n",
    "        return {}\n",
    "    \n",
    "    # First attempt: Try to use the models as specified in lstm_architectures\n",
    "    for architecture in lstm_architectures:\n",
    "        # Get model name\n",
    "        model_name = get_model_name(architecture, alpha, beta)\n",
    "        print(f\"\\nLooking for model: {model_name}\")\n",
    "        \n",
    "        # Try different possible extensions/locations\n",
    "        model_found = False\n",
    "        for ext in ['.keras', '.h5']:\n",
    "            potential_path = f\"{model_name}{ext}\"\n",
    "            \n",
    "            # Check if file exists in the available files\n",
    "            for file_path in available_model_files:\n",
    "                if file_path.endswith(potential_path):\n",
    "                    print(f\"Found matching model: {file_path}\")\n",
    "                    try:\n",
    "                        # Generate forecasts\n",
    "                        forecasts_df = generate_forecasts(df, file_path, preprocessor_path)\n",
    "                        \n",
    "                        # Store in dictionary\n",
    "                        all_forecasts[model_name] = forecasts_df\n",
    "                        \n",
    "                        # Save to CSV\n",
    "                        csv_filename = f'forecasts_{model_name}.csv'\n",
    "                        forecasts_df.to_csv(csv_filename, index=False)\n",
    "                        print(f\"Saved forecasts to {csv_filename}\")\n",
    "                        \n",
    "                        # Display summary\n",
    "                        print(f\"Shape of forecast dataframe: {forecasts_df.shape}\")\n",
    "                        print(\"\\nFirst few forecasts:\")\n",
    "                        print(forecasts_df.head())\n",
    "                        \n",
    "                        model_found = True\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating forecasts for {file_path}: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            if model_found:\n",
    "                break\n",
    "        \n",
    "        if not model_found:\n",
    "            print(f\"Model file for {model_name} not found. Skipping.\")\n",
    "        \n",
    "        # Clear session after each model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # If no models were found using the architecture names, try to use whatever models are available\n",
    "    if not all_forecasts:\n",
    "        print(\"\\nFalling back to using available model files...\")\n",
    "        for model_file in available_model_files:\n",
    "            model_name = os.path.basename(model_file).replace('.keras', '').replace('.h5', '')\n",
    "            print(f\"\\nGenerating forecasts using model: {model_file}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate forecasts\n",
    "                forecasts_df = generate_forecasts(df, model_file, preprocessor_path)\n",
    "                \n",
    "                # Store in dictionary\n",
    "                all_forecasts[model_name] = forecasts_df\n",
    "                \n",
    "                # Save to CSV\n",
    "                csv_filename = f'forecasts_{model_name}.csv'\n",
    "                forecasts_df.to_csv(csv_filename, index=False)\n",
    "                print(f\"Saved forecasts to {csv_filename}\")\n",
    "                \n",
    "                # Display summary\n",
    "                print(f\"Shape of forecast dataframe: {forecasts_df.shape}\")\n",
    "                print(\"\\nFirst few forecasts:\")\n",
    "                print(forecasts_df.head())\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating forecasts for {model_file}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # Clear session after each model\n",
    "            tf.keras.backend.clear_session()\n",
    "    \n",
    "    return all_forecasts\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Read and prepare the dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        df = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        \n",
    "        # Check if dataset contains expected columns\n",
    "        expected_columns = ['Date', 'T', 'V', 'P', 'F']\n",
    "        if not all(col in df.columns for col in expected_columns):\n",
    "            print(f\"Warning: Dataset does not contain all expected columns: {expected_columns}\")\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        df = df[['Date', 'T', 'V', 'P', 'F']]\n",
    "        \n",
    "        # Convert Date to datetime and set as index\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "        # Generate all forecasts\n",
    "        print(\"\\nStarting forecast generation for all models...\")\n",
    "        all_forecasts = generate_all_forecasts(df)\n",
    "        \n",
    "        if all_forecasts:\n",
    "            print(f\"\\nForecast generation completed! Generated forecasts for {len(all_forecasts)} models.\")\n",
    "        else:\n",
    "            print(\"\\nNo forecasts were generated. Please check the model files and try again.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.holoviz.org/panel/1.4.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='4af263b8-d30c-4b82-aa7e-05ab25dc7bf7'>\n",
       "  <div id=\"bfe1842a-d80e-4718-b0c8-bd4a958cdbdd\" data-root-id=\"4af263b8-d30c-4b82-aa7e-05ab25dc7bf7\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"85612b30-120d-47d0-ac74-d13e03633f8d\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"4af263b8-d30c-4b82-aa7e-05ab25dc7bf7\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"85b4fe67-22e5-4d52-999b-0104b71472c3\",\"attributes\":{\"plot_id\":\"4af263b8-d30c-4b82-aa7e-05ab25dc7bf7\",\"comm_id\":\"f6d06d4806f14541a0575fb405fff244\",\"client_comm_id\":\"33f9115ae46e4e2dbb96e5a6aad04979\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"85612b30-120d-47d0-ac74-d13e03633f8d\",\"roots\":{\"4af263b8-d30c-4b82-aa7e-05ab25dc7bf7\":\"bfe1842a-d80e-4718-b0c8-bd4a958cdbdd\"},\"root_ids\":[\"4af263b8-d30c-4b82-aa7e-05ab25dc7bf7\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "4af263b8-d30c-4b82-aa7e-05ab25dc7bf7"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"logo-block\">\n",
       "<img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAB+wAAAfsBxc2miwAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAA6zSURB\n",
       "VHic7ZtpeFRVmsf/5966taWqUlUJ2UioBBJiIBAwCZtog9IOgjqACsogKtqirT2ttt069nQ/zDzt\n",
       "tI4+CrJIREFaFgWhBXpUNhHZQoKBkIUASchWla1S+3ar7r1nPkDaCAnZKoQP/D7mnPOe9/xy76n3\n",
       "nFSAW9ziFoPFNED2LLK5wcyBDObkb8ZkxuaoSYlI6ZcOKq1eWFdedqNzGHQBk9RMEwFAASkk0Xw3\n",
       "ETacDNi2vtvc7L0ROdw0AjoSotQVkKSvHQz/wRO1lScGModBFbDMaNRN1A4tUBCS3lk7BWhQkgpD\n",
       "lG4852/+7DWr1R3uHAZVQDsbh6ZPN7CyxUrCzJMRouusj0ipRwD2uKm0Zn5d2dFwzX1TCGhnmdGo\n",
       "G62Nna+isiUqhkzuKrkQaJlPEv5mFl2fvGg2t/VnzkEV8F5ioioOEWkLG86fvbpthynjdhXYZziQ\n",
       "x1hC9J2NFyi8vCTt91Fh04KGip0AaG9zuCk2wQCVyoNU3Hjezee9bq92duzzTmxsRJoy+jEZZZYo\n",
       "GTKJ6SJngdJqAfRzpze0+jHreUtPc7gpBLQnIYK6BYp/uGhw9YK688eu7v95ysgshcg9qSLMo3JC\n",
       "4jqLKQFBgdKDPoQ+Pltb8dUyQLpeDjeVgI6EgLIQFT5tEl3rn2losHVsexbZ3EyT9wE1uGdkIPcy\n",
       "BGxn8QUq1QrA5nqW5i2tLqvrrM9NK6AdkVIvL9E9bZL/oyfMVd/jqvc8LylzRBKDJSzIExwhQzuL\n",
       "QYGQj4rHfFTc8mUdu3E7yoLtbTe9gI4EqVgVkug2i5+uXGo919ixbRog+3fTbQ8qJe4ZOYNfMoTI\n",
       "OoshUNosgO60AisX15aeI2PSIp5KiFLI9ubb1vV3Qb2ltwLakUCDAkWX7/nHKRmmGIl9VgYsUhJm\n",
       "2NXjKYADtM1ygne9QQDIXlk49FBstMKx66D1v4+XuQr7vqTe0VcBHQlRWiOCbmmSYe2SqtL6q5rJ\n",
       "zsTb7lKx3FKOYC4DoqyS/B5bvLPxvD9Qtf6saxYLQGJErmDOdOMr/zo96km1nElr8bmPOBwI9COv\n",
       "HnFPRIwmkSOv9kcAS4heRsidOkpeWBgZM+UBrTFAXNYL5Vf2ii9c1trNzpYdaoVil3WIc+wdk+gQ\n",
       "noie3ecCcxt9ITcLAPWt/laGEO/9U6PmzZkenTtsSMQ8uYywJVW+grCstAvCIaAdArAsIWkRDDs/\n",
       "KzLm2YcjY1Lv0UdW73HabE9n6V66cxSzfEmuJssTpKGVp+0vHq73FwL46eOjpMpbRAnNmJFrGJNu\n",
       "Ukf9Yrz+3rghiumCKNXXWPhLYcjxGsIpoCMsIRoFITkW8AuyM8jC1+/QLx4bozCEJIq38+1rtpR6\n",
       "V/yzb8eBlRb3fo5l783N0CWolAzJHaVNzkrTzlEp2bQ2q3TC5gn6wpnoQAmwSiGh2GitnTmVMc5O\n",
       "UyfKWUKCIsU7+fZDKwqdT6DDpvkzAX4/+AMFjk0tDp5GRXLpQ2MUmhgDp5gxQT8+Y7hyPsMi8uxF\n",
       "71H0oebujHALECjFKaW9Lm68n18wXp2kVzIcABytD5iXFzg+WVXkegpAsOOYziqo0OkK76GyquC3\n",
       "ltZAzMhhqlSNmmWTE5T6e3IN05ITFLM4GdN0vtZ3ob8Jh1NAKXFbm5PtLU/eqTSlGjkNAJjdgn/N\n",
       "aedXa0tdi7+t9G0FIF49rtMSEgAs1kDLkTPO7ebm4IUWeyh1bKomXqlgMG6kJmHcSM0clYLJ8XtR\n",
       "1GTnbV3F6I5wCGikAb402npp1h1s7LQUZZSMIfALFOuL3UUrfnS8+rez7v9qcold5tilgHbO1fjK\n",
       "9ubb17u9oshxzMiUBKXWqJNxd+fqb0tLVs4lILFnK71H0Ind7uiPgACVcFJlrb0tV6DzxqqTIhUM\n",
       "CwDf1/rrVhTa33/3pGPxJYdQ2l2cbgVcQSosdx8uqnDtbGjh9SlDVSMNWhlnilfqZk42Th2ZpLpf\n",
       "xrHec5e815zrr0dfBZSwzkZfqsv+1FS1KUknUwPARVvItfKUY+cn57yP7qv07UE3p8B2uhUwLk09\n",
       "e0SCOrK+hbdYHYLjRIl71wWzv9jpEoeOHhGRrJAzyEyNiJuUqX0g2sBN5kGK6y2Blp5M3lsB9Qh4\n",
       "y2Ja6x6+i0ucmKgwMATwhSjdUu49tKrQ/pvN5d53ml2CGwCmJipmKjgmyuaXzNeL2a0AkQ01Th5j\n",
       "2DktO3Jyk8f9vcOBQHV94OK+fPumJmvQHxJoWkaKWq9Vs+yUsbq0zGT1I4RgeH2b5wef7+c7bl8F\n",
       "eKgoHVVZa8ZPEORzR6sT1BzDUAD/d9F78e2Tzv99v8D+fLVTqAKAsbGamKey1Mt9Ann4eH3gTXTz\n",
       "idWtAJ8PQWOk7NzSeQn/OTHDuEikVF1R4z8BQCy+6D1aWRfY0tTGG2OM8rRoPaeIj5ZHzJxszElN\n",
       "VM8K8JS5WOfv8mzRnQAKoEhmt8gyPM4lU9SmBK1MCQBnW4KONT86v1hZ1PbwSXPw4JWussVjtH9Y\n",
       "NCoiL9UoH/6PSu8jFrfY2t36erQHXLIEakMi1SydmzB31h3GGXFDFNPaK8Rme9B79Ixrd0WN+1ij\n",
       "NRQ/doRmuFLBkHSTOm5GruG+pFjFdAmorG4IXH1Qua6ASniclfFtDYt+oUjKipPrCQB7QBQ2lrgP\n",
       "fFzm+9XWUtcqJ3/5vDLDpJ79XHZk3u8nGZ42qlj1+ydtbxysCezrydp6ugmipNJ7WBPB5tydY0jP\n",
       "HaVNzs3QzeE4ZpTbI+ZbnSFPbVOw9vsfnVvqWnirPyCNGD08IlqtYkh2hjZ5dErEQzoNm+6ykyOt\n",
       "Lt5/PQEuSRRKo22VkydK+vvS1XEKlhCJAnsqvcVvH7f/ZU2R67eXbMEGAMiIV5oWZWiWvz5Fv2xG\n",
       "sjqNJQRvn3Rs2lji/lNP19VjAQDgD7FHhujZB9OGqYxRkZxixgRDVlqS6uEOFaJUVu0rPFzctrnF\n",
       "JqijImVp8dEKVWyUXDk92zAuMZ6bFwpBU1HrOw6AdhQgUooChb0+ItMbWJitSo5Ws3IAOGEOtL53\n",
       "0vHZih9sC4vtofZ7Qu6523V/fmGcds1TY3V36pUsBwAbSlxnVh2xLfAD/IAIMDf7XYIkNmXfpp2l\n",
       "18rkAJAy9HKFaIr/qULkeQQKy9zf1JgDB2uaeFNGijo5QsUyacNUUTOnGO42xSnv4oOwpDi1zYkc\n",
       "efUc3I5Gk6PhyTuVKaOGyLUAYPGIoY9Pu/atL/L92+4q9wbflRJ2Trpm/jPjdBtfnqB/dIThcl8A\n",
       "KG7hbRuKnb8qsQsVvVlTrwQAQMUlf3kwJI24Z4JhPMtcfng5GcH49GsrxJpGvvHIaeem2ma+KSjQ\n",
       "lIwUdYyCY8j4dE1KzijNnIP2llF2wcXNnsoapw9XxsgYAl6k+KzUXbi2yP3KR2ecf6z3BFsBICdW\n",
       "nvnIaG3eHybqX7vbpEqUMT+9OL4Qpe8VON7dXuFd39v19FoAABRVePbGGuXTszO0P7tu6lghUonE\n",
       "llRdrhArLvmKdh9u29jcFiRRkfLUxBiFNiqSU9icoZQHo5mYBI1MBgBH6wMNb+U7Pnw337H4gi1Y\n",
       "ciWs+uks3Z9fztUvfzxTm9Ne8XXkvQLHNytOOZeiD4e0PgkAIAYCYknKUNUDSXEKzdWNpnil7r4p\n",
       "xqkjTarZMtk/K8TQ6Qve78qqvXurGwIJqcOUKfUWHsm8KGvxSP68YudXq4pcj39X49uOK2X142O0\n",
       "Tz5/u/7TVybqH0rSya6ZBwD21/gubbrgWdDgEOx9WUhfBaC2ibcEBYm7a7x+ukrBMNcEZggyR0TE\n",
       "T8zUPjikQ4VosQZbTpS4vqizBKvqmvjsqnpfzaZyx9JPiz1/bfGKdgD45XB1zoIMzYbfTdS/NClB\n",
       "Gct0USiY3YL/g0LHy/uq/Ef6uo5+n0R/vyhp17Klpge763f8rMu6YU/zrn2nml+2WtH+Z+5IAAFc\n",
       "2bUTdTDOSNa9+cQY7YLsOIXhevEkCvzph7a8laecz/Un/z4/Ae04XeL3UQb57IwU9ZDr9UuKVajv\n",
       "nxp1+1UVIo/LjztZkKH59fO3G/JemqCfmaCRqbqbd90ZZ8FfjtkfAyD0J/9+C2h1hDwsSxvGjNDc\n",
       "b4zk5NfrSwiQblLHzZhg+Jf4aPlUwpDqkQqa9nimbt1/TDH8OitGMaQnj+RJS6B1fbF7SY1TqO5v\n",
       "/v0WAADl1f7zokgS7s7VT2DZ7pegUjBM7mjtiDZbcN4j0YrHH0rXpCtY0qPX0cVL0rv5jv/ZXend\n",
       "0u/EESYBAFBU4T4Qa5TflZOhTe7pmKpaP8kCVUVw1+yhXfJWvn1P3hnXi33JsTN6PnP3hHZ8Z3/h\n",
       "aLHzmkNPuPj7Bc/F/Q38CwjTpSwQXgE4Vmwry9tpfq/ZFgqFMy4AVDtCvi8rvMvOmv0N4YwbVgEA\n",
       "sPM72/KVnzfspmH7HQGCRLG2yL1+z8XwvPcdCbsAANh+xPzstgMtxeGKt+6MK3/tacfvwhWvIwMi\n",
       "oKEBtm0H7W+UVfkc/Y1V0BhoPlDr/w1w/eu1vjIgAgDg22OtX6/eYfnEz/focrZTHAFR+PSs56/7\n",
       "q32nwpjazxgwAQCwcU/T62t3WL7r6/jVRa6/byp1rei+Z98ZUAEAhEPHPc8fKnTU9nbgtnOe8h0l\n",
       "9hcGIqmODLQAHCy2Xti6v/XNRivf43f4fFvIteu854+VHnR7q9tfBlwAAGz+pnndB9vM26UebAe8\n",
       "SLHujPOTPVW+rwY+sxskAAC2HrA8t2Vvc7ffP1r9o+vwR2dcr92InIAbKKC1FZ5tB1tf+/G8p8sv\n",
       "N/9Q5zd/XR34LYCwV5JdccMEAMDBk45DH243r/X4xGvqxFa/GNpS7n6rwOwNWwHVE26oAADYurf1\n",
       "zx/utOzt+DMKYM0p17YtZZ5VNzqfsB2HewG1WXE8PoZ7gOclbTIvynZf9JV+fqZtfgs/8F/Nu5rB\n",
       "EIBmJ+8QRMmpU7EzGRsf2FzuePqYRbzh/zE26EwdrT10f6r6o8HOYzCJB9Dpff8tbnGLG8L/A/WE\n",
       "roTBs2RqAAAAAElFTkSuQmCC'\n",
       "     style='height:25px; border-radius:12px; display: inline-block; float: left; vertical-align: middle'></img>\n",
       "\n",
       "\n",
       "  <img src='data:image/png;base64,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'\n",
       "       style='height:15px; border-radius:12px; display: inline-block; float: left'></img>\n",
       "  \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded MSO forecasts with 100 rows\n",
      "Successfully loaded observed data with 1200 rows\n",
      "Found 28 forecast files:\n",
      " - forecasts_lstm_h2_16_8_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h2_32_16_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_16_8_4_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_32_16_8_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_64_32_16_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_8_4_2_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_all_years\\forecasts_back_loaded_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_bimodal_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_current_amplified_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_current_moderated_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_current_standard_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_front_loaded_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_middle_emphasis_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_spring_emphasis_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_summer_emphasis_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_uniform_1923_2021.csv\n",
      " - forecasts_all_years\\forecasts_winter_emphasis_1923_2021.csv\n",
      " - month_weights_forecasts\\forecasts_back_loaded.csv\n",
      " - month_weights_forecasts\\forecasts_bimodal.csv\n",
      " - month_weights_forecasts\\forecasts_current_amplified.csv\n",
      " - month_weights_forecasts\\forecasts_current_moderated.csv\n",
      " - month_weights_forecasts\\forecasts_current_standard.csv\n",
      " - month_weights_forecasts\\forecasts_front_loaded.csv\n",
      " - month_weights_forecasts\\forecasts_middle_emphasis.csv\n",
      " - month_weights_forecasts\\forecasts_spring_emphasis.csv\n",
      " - month_weights_forecasts\\forecasts_summer_emphasis.csv\n",
      " - month_weights_forecasts\\forecasts_uniform.csv\n",
      " - month_weights_forecasts\\forecasts_winter_emphasis.csv\n",
      "Successfully loaded forecasts for lstm_h3_64_32_16_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h3_32_16_8_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h3_16_8_4_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h3_8_4_2_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h2_32_16_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h2_16_8_a0.1_b0.1_1943_2021\n",
      "\n",
      "Data loading summary:\n",
      "MSO data: âœ“\n",
      "Observed data: âœ“\n",
      "WYT data: âœ“\n",
      "Forecast models: 6\n",
      "Launching server at http://localhost:54794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Legend\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Enable extensions\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Define LSTM architectures for naming (updated to include [8, 4, 2])\n",
    "lstm_architectures = [\n",
    "    [64, 32, 16],  # Original architecture\n",
    "    [32, 16, 8],   # Simpler 3-layer\n",
    "    [16, 8, 4],    # Simpler 3-layer\n",
    "    [8, 4, 2],     # Simpler 3-layer (added)\n",
    "    [32, 16],      # 2-layer\n",
    "    [16, 8],       # 2-layer\n",
    "]\n",
    "\n",
    "# Define the test/training cutoff years\n",
    "TEST_START_YEAR = 1923\n",
    "TEST_END_YEAR = 1942\n",
    "TRAINING_START_YEAR = 1943\n",
    "\n",
    "def get_model_name(architecture):\n",
    "    \"\"\"Generate consistent model name\"\"\"\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"lstm_{arch_str}_a0.1_b0.1_1943_2021\"  # Updated to include 1943_2021 suffix\n",
    "\n",
    "def find_forecast_files():\n",
    "    \"\"\"Find all available forecast CSV files\"\"\"\n",
    "    forecast_files = glob(\"forecasts_*.csv\") + glob(\"*/forecasts_*.csv\")\n",
    "    print(f\"Found {len(forecast_files)} forecast files:\")\n",
    "    for file in forecast_files:\n",
    "        print(f\" - {file}\")\n",
    "    return forecast_files\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all necessary data files\"\"\"\n",
    "    # Status tracking for loaded data\n",
    "    loaded_data = {\n",
    "        \"MSO\": False,\n",
    "        \"Observed\": False,\n",
    "        \"WYT\": False,\n",
    "        \"Forecasts\": 0\n",
    "    }\n",
    "    \n",
    "    # Load MSO data\n",
    "    try:\n",
    "        mso_df = pd.read_csv('MSO_forecast_9.csv')\n",
    "        print(f\"Successfully loaded MSO forecasts with {len(mso_df)} rows\")\n",
    "        loaded_data[\"MSO\"] = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MSO forecasts: {str(e)}\")\n",
    "        print(\"Please make sure 'MSO_forecast_9.csv' exists and contains the expected columns.\")\n",
    "        mso_df = pd.DataFrame()  # Empty dataframe as fallback\n",
    "    \n",
    "    # Load observed data\n",
    "    try:\n",
    "        observed_df = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        # Convert Date to datetime in observed data\n",
    "        observed_df['Date'] = pd.to_datetime(observed_df['Date'])\n",
    "        observed_df['Year'] = observed_df['Date'].dt.year\n",
    "        observed_df['Month'] = observed_df['Date'].dt.month\n",
    "        print(f\"Successfully loaded observed data with {len(observed_df)} rows\")\n",
    "        loaded_data[\"Observed\"] = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading observed data: {str(e)}\")\n",
    "        print(\"Please make sure 'Data/Clean/Orov_clean.csv' exists and contains the expected columns.\")\n",
    "        observed_df = pd.DataFrame(columns=['Date', 'Year', 'Month', 'F'])  # Empty dataframe as fallback\n",
    "    \n",
    "    # Load WYT data\n",
    "    try:\n",
    "        wyt_df = pd.read_csv('WYT.csv')\n",
    "        wyt_df.columns = wyt_df.columns.str.strip().str.lower()  # Normalize column names\n",
    "        if 'year' not in wyt_df.columns or 'wyt' not in wyt_df.columns:\n",
    "            print(\"Warning: WYT file does not have expected columns. Looking for 'year' and 'wyt'\")\n",
    "            print(f\"Available columns: {wyt_df.columns.tolist()}\")\n",
    "        else:\n",
    "            loaded_data[\"WYT\"] = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading WYT data: {str(e)}\")\n",
    "        wyt_df = pd.DataFrame(columns=['year', 'wyt'])\n",
    "    \n",
    "    # Find available forecast files\n",
    "    available_forecasts = find_forecast_files()\n",
    "    \n",
    "    # Load all LSTM forecasts\n",
    "    lstm_forecasts = {}\n",
    "    \n",
    "    # First try to load the files for the specified architectures\n",
    "    for arch in lstm_architectures:\n",
    "        model_name = get_model_name(arch)\n",
    "        forecast_filename = f'forecasts_{model_name}.csv'\n",
    "        \n",
    "        # Check if file exists in current directory or any found files\n",
    "        file_found = False\n",
    "        for available_file in available_forecasts:\n",
    "            if os.path.basename(available_file) == forecast_filename:\n",
    "                forecast_filename = available_file\n",
    "                file_found = True\n",
    "                break\n",
    "        \n",
    "        if not file_found:\n",
    "            print(f\"Warning: Forecast file for {model_name} not found in the expected location.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(forecast_filename)\n",
    "            lstm_forecasts[model_name] = df\n",
    "            loaded_data[\"Forecasts\"] += 1\n",
    "            print(f\"Successfully loaded forecasts for {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {forecast_filename}: {str(e)}\")\n",
    "    \n",
    "    # If no forecasts were loaded using the architecture names, try to load any available forecast files\n",
    "    if not lstm_forecasts and available_forecasts:\n",
    "        print(\"\\nAttempting to load any available forecast files...\")\n",
    "        for file_path in available_forecasts:\n",
    "            try:\n",
    "                # Extract model name from filename\n",
    "                filename = os.path.basename(file_path)\n",
    "                if filename.startswith('forecasts_') and filename.endswith('.csv'):\n",
    "                    model_name = filename[len('forecasts_'):-4]  # Remove 'forecasts_' prefix and '.csv' suffix\n",
    "                    \n",
    "                    df = pd.read_csv(file_path)\n",
    "                    lstm_forecasts[model_name] = df\n",
    "                    loaded_data[\"Forecasts\"] += 1\n",
    "                    print(f\"Successfully loaded forecasts for {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nData loading summary:\")\n",
    "    print(f\"MSO data: {'âœ“' if loaded_data['MSO'] else 'âœ—'}\")\n",
    "    print(f\"Observed data: {'âœ“' if loaded_data['Observed'] else 'âœ—'}\")\n",
    "    print(f\"WYT data: {'âœ“' if loaded_data['WYT'] else 'âœ—'}\")\n",
    "    print(f\"Forecast models: {loaded_data['Forecasts']}\")\n",
    "    \n",
    "    return lstm_forecasts, mso_df, observed_df, wyt_df\n",
    "\n",
    "def calculate_excel_r2(observed_values, forecast_values):\n",
    "    \"\"\"Calculate R-squared like Excel's CORREL^2\"\"\"\n",
    "    try:\n",
    "        if len(observed_values) != len(forecast_values):\n",
    "            return None\n",
    "            \n",
    "        f_mean = np.mean(forecast_values)\n",
    "        o_mean = np.mean(observed_values)\n",
    "        \n",
    "        numerator = sum((f - f_mean) * (o - o_mean) \n",
    "                        for f, o in zip(forecast_values, observed_values))\n",
    "        \n",
    "        f_variance = sum((f - f_mean) ** 2 for f in forecast_values)\n",
    "        o_variance = sum((o - o_mean) ** 2 for o in observed_values)\n",
    "        \n",
    "        r = numerator / np.sqrt(f_variance * o_variance)\n",
    "        return r ** 2\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Excel R2: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_custom_loss(observed_values, forecast_values, alpha=0.1, beta=0.1):\n",
    "    \"\"\"Calculate the custom loss function with smoothness penalty and monthly weights\"\"\"\n",
    "    try:\n",
    "        month_weights = np.array([2, 2, 2, 1.5, 1.5, 1, 1, 1, 1])\n",
    "        \n",
    "        # MSE component\n",
    "        mse_loss = np.mean((observed_values - forecast_values) ** 2)\n",
    "        \n",
    "        # Smoothness penalty\n",
    "        smoothness_penalty = np.mean(np.square(forecast_values[1:] - forecast_values[:-1]))\n",
    "        \n",
    "        # Monthly weighted loss\n",
    "        monthly_weight_loss = np.mean(month_weights * np.square(observed_values - forecast_values))\n",
    "        \n",
    "        return mse_loss + alpha * smoothness_penalty + beta * monthly_weight_loss\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating custom loss: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_percentage_bias(observed_values, forecast_values):\n",
    "    \"\"\"Calculate percentage bias\"\"\"\n",
    "    try:\n",
    "        return 100 * (np.mean(forecast_values) - np.mean(observed_values)) / np.mean(observed_values)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating percentage bias: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_comparison_plot(lstm_forecasts, mso_df, observed_df, selected_year, wyt_value=None, is_training=None):\n",
    "    \"\"\"Create comparison plot with all models using direct Bokeh implementation\"\"\"\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "    month_nums = list(range(1, 10))\n",
    "    \n",
    "    # Check if required data is available\n",
    "    if observed_df.empty:\n",
    "        return pn.pane.Markdown(f\"**Error:** Observed data not loaded. Cannot create plot for year {selected_year}.\")\n",
    "    \n",
    "    # Create a Bokeh figure\n",
    "    title = f'Flow Comparison for Year {selected_year}'\n",
    "    if wyt_value:\n",
    "        title += f' (WYT = {wyt_value})'\n",
    "    \n",
    "    # Add period indicator to title\n",
    "    if is_training is not None:\n",
    "        period_label = \"Training Period\" if is_training else \"Test Period\"\n",
    "        title += f' - {period_label}'\n",
    "    \n",
    "    # Create the plot\n",
    "    try:\n",
    "        p = figure(\n",
    "            width=900, \n",
    "            height=500,\n",
    "            title=title,\n",
    "            tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
    "            x_range=[0.5, 9.5],  # Slightly expanded range for better visibility\n",
    "            toolbar_location=\"above\"\n",
    "        )\n",
    "        \n",
    "        # Set background color based on period\n",
    "        if is_training is not None:\n",
    "            if is_training:\n",
    "                p.background_fill_color = \"#f5f5ff\"  # Light blue for training\n",
    "            else:\n",
    "                p.background_fill_color = \"#fff5f5\"  # Light red for test\n",
    "        \n",
    "        # Set up x-axis ticks with month names\n",
    "        p.xaxis.ticker = month_nums\n",
    "        p.xaxis.major_label_overrides = {i: m for i, m in zip(month_nums, months)}\n",
    "        \n",
    "        # Create hover tool\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"Series\", \"$name\"),\n",
    "                (\"Month\", \"@month\"),\n",
    "                (\"Value\", \"@value{0.0}\")\n",
    "            ],\n",
    "            line_policy='nearest',\n",
    "            point_policy='snap_to_data',\n",
    "            mode='mouse'\n",
    "        )\n",
    "        p.add_tools(hover)\n",
    "        \n",
    "        legend_items = []\n",
    "        \n",
    "        # Add MSO forecast if available\n",
    "        if not mso_df.empty and selected_year in mso_df['Year'].values:\n",
    "            try:\n",
    "                mso_forecast = mso_df[mso_df['Year'] == selected_year].iloc[0]\n",
    "                mso_values = [float(mso_forecast[month]) for month in months]\n",
    "                \n",
    "                # Create MSO data source\n",
    "                source_mso = ColumnDataSource(data=dict(\n",
    "                    x=month_nums,\n",
    "                    value=mso_values,\n",
    "                    month=months\n",
    "                ))\n",
    "                \n",
    "                # Create MSO line\n",
    "                r_mso = p.line('x', 'value', source=source_mso, \n",
    "                              line_width=2, color='green', line_dash='dotted')\n",
    "                r_mso.name = \"MSO\"  # Set name for hover tool\n",
    "                \n",
    "                # Add to legend\n",
    "                legend_items.append((\"MSO\", [r_mso]))\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding MSO forecast for year {selected_year}: {str(e)}\")\n",
    "        \n",
    "        # Get observed data for selected year\n",
    "        observed_data = observed_df[\n",
    "            (observed_df['Year'] == selected_year) & \n",
    "            (observed_df['Month'].isin(range(1, 10)))\n",
    "        ]\n",
    "        \n",
    "        # Add observed data if available\n",
    "        if not observed_data.empty:\n",
    "            try:\n",
    "                # Prepare observed values (handle missing months)\n",
    "                observed_values = []\n",
    "                for m in month_nums:\n",
    "                    month_data = observed_data[observed_data['Month'] == m]\n",
    "                    if not month_data.empty:\n",
    "                        observed_values.append(float(month_data['F'].values[0]))\n",
    "                    else:\n",
    "                        observed_values.append(float('nan'))\n",
    "                \n",
    "                # Create observed data source\n",
    "                source_obs = ColumnDataSource(data=dict(\n",
    "                    x=month_nums,\n",
    "                    value=observed_values,\n",
    "                    month=months\n",
    "                ))\n",
    "                \n",
    "                # Create observed line\n",
    "                r_obs = p.line('x', 'value', source=source_obs, \n",
    "                              line_width=3, color='red')\n",
    "                r_obs.name = \"Observed\"  # Set name for hover tool\n",
    "                \n",
    "                # Add to legend\n",
    "                legend_items.append((\"Observed\", [r_obs]))\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding observed data for year {selected_year}: {str(e)}\")\n",
    "        \n",
    "        # Add LSTM models\n",
    "        if lstm_forecasts:\n",
    "            colors = plt.cm.rainbow(np.linspace(0, 1, len(lstm_forecasts)))\n",
    "            \n",
    "            for (model_name, lstm_df), color in zip(lstm_forecasts.items(), colors):\n",
    "                try:\n",
    "                    if selected_year not in lstm_df['Year'].values:\n",
    "                        print(f\"Warning: Year {selected_year} not found in {model_name} forecasts\")\n",
    "                        continue\n",
    "                        \n",
    "                    lstm_forecast = lstm_df[lstm_df['Year'] == selected_year].iloc[0]\n",
    "                    \n",
    "                    # Check if F_month columns exist\n",
    "                    f_columns = [f'F_{month}' for month in months]\n",
    "                    missing_columns = [col for col in f_columns if col not in lstm_forecast.index]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: {model_name} is missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "                        \n",
    "                    lstm_values = [float(lstm_forecast[f'F_{m}']) for m in months]\n",
    "                    \n",
    "                    # Create LSTM data source\n",
    "                    source_lstm = ColumnDataSource(data=dict(\n",
    "                        x=month_nums,\n",
    "                        value=lstm_values,\n",
    "                        month=months\n",
    "                    ))\n",
    "                    \n",
    "                    # Create LSTM line with unique color\n",
    "                    hex_color = rgb_to_hex(color)\n",
    "                    r_lstm = p.line('x', 'value', source=source_lstm, \n",
    "                                  line_width=2, color=hex_color, line_dash='dashed')\n",
    "                    r_lstm.name = model_name  # Set name for hover tool\n",
    "                    \n",
    "                    # Add to legend\n",
    "                    legend_items.append((model_name, [r_lstm]))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding {model_name} for year {selected_year}: {str(e)}\")\n",
    "        \n",
    "        # Add legend if we have any items\n",
    "        if legend_items:\n",
    "            legend = Legend(items=legend_items)\n",
    "            legend.click_policy = \"hide\"  # Allow toggling lines by clicking legend\n",
    "            p.add_layout(legend, 'right')\n",
    "        \n",
    "        # Format the plot\n",
    "        p.grid.grid_line_alpha = 0.3\n",
    "        p.y_range.start = 0  # Start y-axis at 0\n",
    "        p.yaxis.axis_label = \"Flow\"\n",
    "        p.xaxis.axis_label = \"Month\"\n",
    "        \n",
    "        # Create a Panel pane from the Bokeh figure\n",
    "        return pn.pane.Bokeh(p)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error creating plot for year {selected_year}: {str(e)}\"\n",
    "        print(error_message)\n",
    "        return pn.pane.Markdown(f\"**Error:** {error_message}\")\n",
    "\n",
    "def calculate_all_metrics(lstm_forecasts, mso_df, observed_df, selected_year):\n",
    "    \"\"\"Calculate all metrics for each model\"\"\"\n",
    "    try:\n",
    "        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "        metrics_text = []\n",
    "        \n",
    "        # Check if observed data is available\n",
    "        observed_data = observed_df[\n",
    "            (observed_df['Year'] == selected_year) & \n",
    "            (observed_df['Month'].isin(range(1, 10)))\n",
    "        ]\n",
    "        \n",
    "        if observed_data.empty:\n",
    "            return \"**Metrics:** No observed data available for this year.\"\n",
    "        \n",
    "        observed_values = np.array(observed_data['F'].values)\n",
    "        \n",
    "        # Check if we have complete data (9 months)\n",
    "        if len(observed_values) != 9:\n",
    "            return f\"**Metrics:** Incomplete observed data for this year ({len(observed_values)}/9 months).\"\n",
    "        \n",
    "        # Calculate MSO metrics if available\n",
    "        if not mso_df.empty and selected_year in mso_df['Year'].values:\n",
    "            try:\n",
    "                mso_forecast = mso_df[mso_df['Year'] == selected_year].iloc[0]\n",
    "                mso_values = np.array([float(mso_forecast[month]) for month in months])\n",
    "                \n",
    "                mso_metrics = {\n",
    "                    'Excel RÂ²': calculate_excel_r2(observed_values, mso_values),\n",
    "                    'Sklearn RÂ²': r2_score(observed_values, mso_values),\n",
    "                    'MAE': mean_absolute_error(observed_values, mso_values),\n",
    "                    'Custom Loss': calculate_custom_loss(observed_values, mso_values),\n",
    "                    'Percentage Bias': calculate_percentage_bias(observed_values, mso_values)\n",
    "                }\n",
    "                \n",
    "                # Filter out None values\n",
    "                mso_metrics = {k: v for k, v in mso_metrics.items() if v is not None}\n",
    "                \n",
    "                if mso_metrics:\n",
    "                    metrics_text.append(\"**MSO Metrics:**\\n\" + \n",
    "                                       \" | \".join([f\"{k}: {v:.2f}\" for k, v in mso_metrics.items()]))\n",
    "                else:\n",
    "                    metrics_text.append(\"**MSO Metrics:** Error calculating metrics.\")\n",
    "            except Exception as e:\n",
    "                metrics_text.append(f\"**MSO Metrics:** Error: {str(e)}\")\n",
    "        \n",
    "        # Calculate LSTM metrics\n",
    "        for model_name, lstm_df in lstm_forecasts.items():\n",
    "            try:\n",
    "                # Skip if this year doesn't exist in the forecast data\n",
    "                if selected_year not in lstm_df['Year'].values:\n",
    "                    metrics_text.append(f\"**{model_name} Metrics:** No forecast available for this year.\")\n",
    "                    continue\n",
    "                \n",
    "                lstm_forecast = lstm_df[lstm_df['Year'] == selected_year].iloc[0]\n",
    "                \n",
    "                # Check if F_month columns exist\n",
    "                f_columns = [f'F_{month}' for month in months]\n",
    "                if not all(col in lstm_forecast.index for col in f_columns):\n",
    "                    metrics_text.append(f\"**{model_name} Metrics:** Incomplete forecast data for this year.\")\n",
    "                    continue\n",
    "                \n",
    "                lstm_values = np.array([float(lstm_forecast[f'F_{month}']) for month in months])\n",
    "                \n",
    "                model_metrics = {\n",
    "                    'Excel RÂ²': calculate_excel_r2(observed_values, lstm_values),\n",
    "                    'Sklearn RÂ²': r2_score(observed_values, lstm_values),\n",
    "                    'MAE': mean_absolute_error(observed_values, lstm_values),\n",
    "                    'Custom Loss': calculate_custom_loss(observed_values, lstm_values),\n",
    "                    'Percentage Bias': calculate_percentage_bias(observed_values, lstm_values)\n",
    "                }\n",
    "                \n",
    "                # Filter out None values\n",
    "                model_metrics = {k: v for k, v in model_metrics.items() if v is not None}\n",
    "                \n",
    "                if model_metrics:\n",
    "                    metrics_text.append(f\"**{model_name} Metrics:**\\n\" + \n",
    "                                      \" | \".join([f\"{k}: {v:.2f}\" for k, v in model_metrics.items()]))\n",
    "                else:\n",
    "                    metrics_text.append(f\"**{model_name} Metrics:** Error calculating metrics.\")\n",
    "            except Exception as e:\n",
    "                metrics_text.append(f\"**{model_name} Metrics:** Error: {str(e)}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(metrics_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Detailed error calculating metrics: {str(e)}\")\n",
    "        return f\"**Metrics:** Error calculating metrics: {str(e)}\"\n",
    "\n",
    "def rgb_to_hex(rgba_color):\n",
    "    \"\"\"Convert RGB(A) color to hex\"\"\"\n",
    "    try:\n",
    "        rgb = tuple(int(x * 255) for x in rgba_color[:3])\n",
    "        return '#{:02x}{:02x}{:02x}'.format(*rgb)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting color: {str(e)}\")\n",
    "        return '#000000'  # Default to black on error\n",
    "\n",
    "def get_common_years(lstm_forecasts, mso_df, observed_df):\n",
    "    \"\"\"Get years common to all datasets, with fallbacks for missing data\"\"\"\n",
    "    try:\n",
    "        # Start with all years from observed data\n",
    "        if not observed_df.empty and 'Year' in observed_df.columns:\n",
    "            all_years = set(observed_df['Year'].unique())\n",
    "        else:\n",
    "            # Fallback to years 1921-2021 if no observed data\n",
    "            all_years = set(range(1921, 2022))\n",
    "        \n",
    "        # Intersect with MSO years if available\n",
    "        if not mso_df.empty and 'Year' in mso_df.columns:\n",
    "            mso_years = set(mso_df['Year'])\n",
    "            all_years = all_years.intersection(mso_years)\n",
    "        \n",
    "        # Intersect with LSTM years if available\n",
    "        if lstm_forecasts:\n",
    "            for model_name, df in lstm_forecasts.items():\n",
    "                if 'Year' in df.columns:\n",
    "                    model_years = set(df['Year'].unique())\n",
    "                    all_years = all_years.intersection(model_years)\n",
    "        \n",
    "        # Sort years for display\n",
    "        return sorted(list(all_years))\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting common years: {str(e)}\")\n",
    "        # Fallback to limited year range if error occurs\n",
    "        return list(range(1923, 2022))\n",
    "\n",
    "def is_test_year(year):\n",
    "    \"\"\"Check if a year is in the test period (1923-1942)\"\"\"\n",
    "    return TEST_START_YEAR <= year <= TEST_END_YEAR\n",
    "\n",
    "def create_dashboard():\n",
    "    \"\"\"Create the main dashboard\"\"\"\n",
    "    # Load data\n",
    "    lstm_forecasts, mso_df, observed_df, wyt_df = load_data()\n",
    "    \n",
    "    # Create containers\n",
    "    plots_container = pn.Column()\n",
    "    \n",
    "    # Get common years across all models (with error handling)\n",
    "    years = get_common_years(lstm_forecasts, mso_df, observed_df)\n",
    "    \n",
    "    # Inform if no years found\n",
    "    if not years:\n",
    "        plots_container.append(pn.pane.Markdown(\"**Error:** No common years found across datasets.\"))\n",
    "        years = list(range(1923, 2022))  # Fallback for UI to work\n",
    "    \n",
    "    # Split years into training and test sets\n",
    "    test_years = [y for y in years if is_test_year(y)]\n",
    "    training_years = [y for y in years if y >= TRAINING_START_YEAR]\n",
    "    \n",
    "    # Prepare water year type options\n",
    "    wyt_options = ['C', 'D', 'BN', 'AN', 'W']\n",
    "    if not wyt_df.empty and 'wyt' in wyt_df.columns:\n",
    "        available_wyt = sorted(wyt_df['wyt'].unique())\n",
    "        if len(available_wyt) > 0:\n",
    "            wyt_options = available_wyt\n",
    "    \n",
    "    # Create period selector\n",
    "    period_select = pn.widgets.RadioButtonGroup(\n",
    "        name='Dataset Period',\n",
    "        options=['All', 'Training', 'Test'],\n",
    "        value='All',\n",
    "        button_type='success'\n",
    "    )\n",
    "    \n",
    "    # Create water year type selector\n",
    "    wyt_select = pn.widgets.MultiChoice(\n",
    "        name='Select Water Year Types',\n",
    "        options=wyt_options,\n",
    "        value=[],\n",
    "        height=100\n",
    "    )\n",
    "    \n",
    "    # Create year selector (will be updated based on WYT and period selection)\n",
    "    year_select = pn.widgets.MultiChoice(\n",
    "        name='Select Years',\n",
    "        options=list(map(str, years)),\n",
    "        value=[str(years[-1])] if years else [],\n",
    "        height=300\n",
    "    )\n",
    "    \n",
    "    # Create data status indicator (without style parameter)\n",
    "    data_status_text = f\"\"\"\n",
    "    ### Data Status:\n",
    "    - **Observed Data:** {'âœ“ Loaded' if not observed_df.empty else 'âœ— Missing'}\n",
    "    - **MSO Forecasts:** {'âœ“ Loaded' if not mso_df.empty else 'âœ— Missing'}\n",
    "    - **WYT Data:** {'âœ“ Loaded' if not wyt_df.empty else 'âœ— Missing'}\n",
    "    - **LSTM Models:** {'âœ“ Loaded ' + str(len(lstm_forecasts)) + ' models' if lstm_forecasts else 'âœ— Missing'}\n",
    "    \"\"\"\n",
    "    data_status = pn.pane.Markdown(data_status_text)\n",
    "    \n",
    "    def update_year_options(event):\n",
    "        # Get selected period\n",
    "        selected_period = period_select.value\n",
    "        \n",
    "        # Filter years based on period\n",
    "        if selected_period == 'Training':\n",
    "            period_filtered_years = training_years\n",
    "        elif selected_period == 'Test':\n",
    "            period_filtered_years = test_years\n",
    "        else:  # 'All'\n",
    "            period_filtered_years = years\n",
    "        \n",
    "        # Filter years based on selected water year types\n",
    "        selected_wyt = wyt_select.value\n",
    "        \n",
    "        if not selected_wyt or wyt_df.empty or 'wyt' not in wyt_df.columns:\n",
    "            # If no WYT selected or no WYT data, show all years for the selected period\n",
    "            filtered_years = period_filtered_years\n",
    "        else:\n",
    "            try:\n",
    "                # Filter years by selected WYT\n",
    "                wyt_filtered_years = wyt_df[wyt_df['wyt'].isin(selected_wyt)]['year'].astype(int).tolist()\n",
    "                # Only keep years that exist in our dataset and the selected period\n",
    "                filtered_years = sorted(list(set(wyt_filtered_years).intersection(set(period_filtered_years))))\n",
    "            except Exception as e:\n",
    "                print(f\"Error filtering years by WYT: {str(e)}\")\n",
    "                filtered_years = period_filtered_years\n",
    "        \n",
    "        # Update year selector options\n",
    "        year_select.options = list(map(str, filtered_years))\n",
    "        \n",
    "        # If current selection is not in filtered options, select the most recent year\n",
    "        if not set(year_select.value).intersection(set(year_select.options)) and year_select.options:\n",
    "            year_select.value = [year_select.options[-1]]\n",
    "        elif not year_select.options:\n",
    "            year_select.value = []\n",
    "    \n",
    "    def update_plots(event):\n",
    "        # Clear the container\n",
    "        plots_container.clear()\n",
    "        \n",
    "        # Add warning if data is missing\n",
    "        if observed_df.empty:\n",
    "            plots_container.append(pn.pane.Markdown(\"âš ï¸ **Warning:** Observed data is missing. Plots may not display correctly.\"))\n",
    "        \n",
    "        if not lstm_forecasts:\n",
    "            plots_container.append(pn.pane.Markdown(\"âš ï¸ **Warning:** No forecast models were found. Please run the forecast generation code first.\"))\n",
    "        \n",
    "        # Get selected period for visual indication\n",
    "        selected_period = period_select.value\n",
    "        \n",
    "        # Handle no years selected\n",
    "        if not year_select.value:\n",
    "            plots_container.append(pn.pane.Markdown(\"**No years selected.** Please select at least one year to display.\"))\n",
    "            return\n",
    "        \n",
    "        # Create plots for each selected year\n",
    "        for year_str in year_select.value:\n",
    "            try:\n",
    "                year = int(year_str)\n",
    "                \n",
    "                # Determine if this year is in training or test set\n",
    "                is_training = None\n",
    "                if selected_period != 'All':\n",
    "                    is_training = not is_test_year(year)  # Year is in training set if not in test period\n",
    "                \n",
    "                # Get WYT for this year if available\n",
    "                wyt_value = None\n",
    "                if not wyt_df.empty and 'year' in wyt_df.columns and 'wyt' in wyt_df.columns:\n",
    "                    year_wyt = wyt_df[wyt_df['year'] == year]['wyt'].values\n",
    "                    if len(year_wyt) > 0:\n",
    "                        wyt_value = year_wyt[0]\n",
    "                \n",
    "                # Create plot with WYT in title if available\n",
    "                plot = create_comparison_plot(lstm_forecasts, mso_df, observed_df, year, wyt_value, is_training)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics_text = calculate_all_metrics(lstm_forecasts, mso_df, observed_df, year)\n",
    "                \n",
    "                # Create period indicator without style parameter\n",
    "                if selected_period != 'All':\n",
    "                    period_indicator = \"## <span style='color:blue;'>Training Period (â‰¥ 1943)</span>\" if is_training else \"## <span style='color:red;'>Test Period (1923-1942)</span>\"\n",
    "                else:\n",
    "                    period_indicator = \"## <span style='color:blue;'>Training Period (â‰¥ 1943)</span>\" if year >= TRAINING_START_YEAR else \"## <span style='color:red;'>Test Period (1923-1942)</span>\"\n",
    "                \n",
    "                # Add to container (using HTML in Markdown instead of style parameter)\n",
    "                plots_container.extend([\n",
    "                    pn.pane.Markdown(period_indicator),\n",
    "                    plot,\n",
    "                    pn.pane.Markdown(metrics_text),\n",
    "                    pn.layout.Divider()\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                plots_container.append(pn.pane.Markdown(f\"**Error creating plot for year {year_str}:** {str(e)}\"))\n",
    "    \n",
    "    # Set up the callbacks\n",
    "    period_select.param.watch(update_year_options, 'value')\n",
    "    wyt_select.param.watch(update_year_options, 'value')\n",
    "    year_select.param.watch(update_plots, 'value')\n",
    "    \n",
    "    # Initial update\n",
    "    update_year_options(None)\n",
    "    update_plots(None)\n",
    "    \n",
    "    # Create period stats pane to show counts (without style parameter)\n",
    "    period_stats_text = f\"\"\"\n",
    "    ### Dataset Period Counts:\n",
    "    - **Training Period (â‰¥ 1943):** {len(training_years)} years\n",
    "    - **Test Period (1923-1942):** {len(test_years)} years\n",
    "    - **Total:** {len(years)} years\n",
    "    \"\"\"\n",
    "    period_stats_pane = pn.pane.Markdown(period_stats_text)\n",
    "    \n",
    "    # Create layout\n",
    "    dashboard = pn.Column(\n",
    "        \"## Flow Forecast Comparison\",\n",
    "        pn.Row(\n",
    "            pn.Column(\n",
    "                \"### Select Dataset Period\", \n",
    "                period_select,\n",
    "                period_stats_pane,\n",
    "                data_status,\n",
    "                \"### Select Water Year Types\", \n",
    "                wyt_select,\n",
    "                \"### Select Years\", \n",
    "                year_select, \n",
    "                width=300\n",
    "            ),\n",
    "            plots_container\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return dashboard\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and display the dashboard\n",
    "    dashboard = create_dashboard()\n",
    "    pn.serve(dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "Successfully loaded MSO forecasts with 100 rows\n",
      "Successfully loaded observed data with 1200 rows\n",
      "Found 6 forecast files:\n",
      " - forecasts_lstm_h2_16_8_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h2_32_16_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_16_8_4_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_32_16_8_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_64_32_16_a0.1_b0.1_1943_2021.csv\n",
      " - forecasts_lstm_h3_8_4_2_a0.1_b0.1_1943_2021.csv\n",
      "Successfully loaded forecasts for lstm_h3_64_32_16_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h3_32_16_8_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h3_16_8_4_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h3_8_4_2_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h2_32_16_a0.1_b0.1_1943_2021\n",
      "Successfully loaded forecasts for lstm_h2_16_8_a0.1_b0.1_1943_2021\n",
      "\n",
      "Data loading summary:\n",
      "MSO data: âœ“ Loaded\n",
      "Observed data: âœ“ Loaded\n",
      "WYT data: âœ“ Loaded\n",
      "Forecast models: 6\n",
      "\n",
      "Calculating metrics for all years and models...\n",
      "Skipping year 1921: Incomplete observed data (0/9 months)\n",
      "Successfully calculated metrics for 694 year/model combinations\n",
      "Metrics saved to all_metrics_summary.csv\n",
      "Summary statistics saved to metrics_summary_by_model_period.csv\n",
      "WYT summary statistics saved to metrics_summary_by_wyt.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define constants\n",
    "TEST_START_YEAR = 1923\n",
    "TEST_END_YEAR = 1942\n",
    "TRAINING_START_YEAR = 1943\n",
    "\n",
    "# Define LSTM architectures for naming (updated to include [8, 4, 2])\n",
    "lstm_architectures = [\n",
    "    [64, 32, 16],  # Original architecture\n",
    "    [32, 16, 8],   # Simpler 3-layer\n",
    "    [16, 8, 4],    # Simpler 3-layer\n",
    "    [8, 4, 2],     # Simpler 3-layer (added)\n",
    "    [32, 16],      # 2-layer\n",
    "    [16, 8],       # 2-layer\n",
    "]\n",
    "\n",
    "def get_model_name(architecture):\n",
    "    \"\"\"Generate consistent model name\"\"\"\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"lstm_{arch_str}_a0.1_b0.1_1943_2021\"\n",
    "\n",
    "def find_forecast_files():\n",
    "    \"\"\"Find all available forecast CSV files\"\"\"\n",
    "    forecast_files = glob(\"forecasts_*.csv\") + glob(\"*/forecasts_*.csv\")\n",
    "    print(f\"Found {len(forecast_files)} forecast files:\")\n",
    "    for file in forecast_files:\n",
    "        print(f\" - {file}\")\n",
    "    return forecast_files\n",
    "\n",
    "def is_test_year(year):\n",
    "    \"\"\"Check if a year is in the test period (1923-1942)\"\"\"\n",
    "    return TEST_START_YEAR <= year <= TEST_END_YEAR\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all necessary data files\"\"\"\n",
    "    # Status tracking for loaded data\n",
    "    data_files = {\n",
    "        \"MSO\": None,\n",
    "        \"Observed\": None,\n",
    "        \"WYT\": None,\n",
    "        \"Forecasts\": {}\n",
    "    }\n",
    "    \n",
    "    # Load MSO data\n",
    "    try:\n",
    "        mso_df = pd.read_csv('MSO_forecast_9.csv')\n",
    "        print(f\"Successfully loaded MSO forecasts with {len(mso_df)} rows\")\n",
    "        data_files[\"MSO\"] = mso_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MSO forecasts: {str(e)}\")\n",
    "        print(\"Please make sure 'MSO_forecast_9.csv' exists and contains the expected columns.\")\n",
    "        data_files[\"MSO\"] = pd.DataFrame()  # Empty dataframe as fallback\n",
    "    \n",
    "    # Load observed data\n",
    "    try:\n",
    "        observed_df = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        # Convert Date to datetime in observed data\n",
    "        observed_df['Date'] = pd.to_datetime(observed_df['Date'])\n",
    "        observed_df['Year'] = observed_df['Date'].dt.year\n",
    "        observed_df['Month'] = observed_df['Date'].dt.month\n",
    "        print(f\"Successfully loaded observed data with {len(observed_df)} rows\")\n",
    "        data_files[\"Observed\"] = observed_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading observed data: {str(e)}\")\n",
    "        print(\"Please make sure 'Data/Clean/Orov_clean.csv' exists and contains the expected columns.\")\n",
    "        data_files[\"Observed\"] = pd.DataFrame(columns=['Date', 'Year', 'Month', 'F'])\n",
    "    \n",
    "    # Load WYT data\n",
    "    try:\n",
    "        wyt_df = pd.read_csv('WYT.csv')\n",
    "        wyt_df.columns = wyt_df.columns.str.strip().str.lower()  # Normalize column names\n",
    "        if 'year' not in wyt_df.columns or 'wyt' not in wyt_df.columns:\n",
    "            print(\"Warning: WYT file does not have expected columns. Looking for 'year' and 'wyt'\")\n",
    "            print(f\"Available columns: {wyt_df.columns.tolist()}\")\n",
    "            data_files[\"WYT\"] = pd.DataFrame(columns=['year', 'wyt'])\n",
    "        else:\n",
    "            data_files[\"WYT\"] = wyt_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading WYT data: {str(e)}\")\n",
    "        data_files[\"WYT\"] = pd.DataFrame(columns=['year', 'wyt'])\n",
    "    \n",
    "    # Find available forecast files\n",
    "    available_forecasts = find_forecast_files()\n",
    "    \n",
    "    # Load all LSTM forecasts\n",
    "    lstm_forecasts = {}\n",
    "    \n",
    "    # First try to load the files for the specified architectures\n",
    "    for arch in lstm_architectures:\n",
    "        model_name = get_model_name(arch)\n",
    "        forecast_filename = f'forecasts_{model_name}.csv'\n",
    "        \n",
    "        # Check if file exists in current directory or any found files\n",
    "        file_found = False\n",
    "        for available_file in available_forecasts:\n",
    "            if os.path.basename(available_file) == forecast_filename:\n",
    "                forecast_filename = available_file\n",
    "                file_found = True\n",
    "                break\n",
    "        \n",
    "        if not file_found:\n",
    "            print(f\"Warning: Forecast file for {model_name} not found in the expected location.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(forecast_filename)\n",
    "            lstm_forecasts[model_name] = df\n",
    "            print(f\"Successfully loaded forecasts for {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {forecast_filename}: {str(e)}\")\n",
    "    \n",
    "    # If no forecasts were loaded using the architecture names, try to load any available forecast files\n",
    "    if not lstm_forecasts and available_forecasts:\n",
    "        print(\"\\nAttempting to load any available forecast files...\")\n",
    "        for file_path in available_forecasts:\n",
    "            try:\n",
    "                # Extract model name from filename\n",
    "                filename = os.path.basename(file_path)\n",
    "                if filename.startswith('forecasts_') and filename.endswith('.csv'):\n",
    "                    model_name = filename[len('forecasts_'):-4]  # Remove 'forecasts_' prefix and '.csv' suffix\n",
    "                    \n",
    "                    df = pd.read_csv(file_path)\n",
    "                    lstm_forecasts[model_name] = df\n",
    "                    print(f\"Successfully loaded forecasts for {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    data_files[\"Forecasts\"] = lstm_forecasts\n",
    "    \n",
    "    print(f\"\\nData loading summary:\")\n",
    "    print(f\"MSO data: {'âœ“ Loaded' if not data_files['MSO'].empty else 'âœ— Missing'}\")\n",
    "    print(f\"Observed data: {'âœ“ Loaded' if not data_files['Observed'].empty else 'âœ— Missing'}\")\n",
    "    print(f\"WYT data: {'âœ“ Loaded' if not data_files['WYT'].empty else 'âœ— Missing'}\")\n",
    "    print(f\"Forecast models: {len(data_files['Forecasts'])}\")\n",
    "    \n",
    "    return data_files\n",
    "\n",
    "def calculate_excel_r2(observed_values, forecast_values):\n",
    "    \"\"\"Calculate R-squared like Excel's CORREL^2\"\"\"\n",
    "    try:\n",
    "        if len(observed_values) != len(forecast_values):\n",
    "            return None\n",
    "            \n",
    "        f_mean = np.mean(forecast_values)\n",
    "        o_mean = np.mean(observed_values)\n",
    "        \n",
    "        numerator = sum((f - f_mean) * (o - o_mean) \n",
    "                        for f, o in zip(forecast_values, observed_values))\n",
    "        \n",
    "        f_variance = sum((f - f_mean) ** 2 for f in forecast_values)\n",
    "        o_variance = sum((o - o_mean) ** 2 for o in observed_values)\n",
    "        \n",
    "        r = numerator / np.sqrt(f_variance * o_variance)\n",
    "        return r ** 2\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Excel R2: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_custom_loss(observed_values, forecast_values, alpha=0.1, beta=0.1):\n",
    "    \"\"\"Calculate the custom loss function with smoothness penalty and monthly weights\"\"\"\n",
    "    try:\n",
    "        month_weights = np.array([2, 2, 2, 1.5, 1.5, 1, 1, 1, 1])\n",
    "        \n",
    "        # MSE component\n",
    "        mse_loss = np.mean((observed_values - forecast_values) ** 2)\n",
    "        \n",
    "        # Smoothness penalty\n",
    "        smoothness_penalty = np.mean(np.square(forecast_values[1:] - forecast_values[:-1]))\n",
    "        \n",
    "        # Monthly weighted loss\n",
    "        monthly_weight_loss = np.mean(month_weights * np.square(observed_values - forecast_values))\n",
    "        \n",
    "        return mse_loss + alpha * smoothness_penalty + beta * monthly_weight_loss\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating custom loss: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_percentage_bias(observed_values, forecast_values):\n",
    "    \"\"\"Calculate percentage bias\"\"\"\n",
    "    try:\n",
    "        return 100 * (np.mean(forecast_values) - np.mean(observed_values)) / np.mean(observed_values)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating percentage bias: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_metrics_for_all_years(data_files):\n",
    "    \"\"\"Calculate metrics for all years and all models\"\"\"\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "    \n",
    "    # Extract data\n",
    "    observed_df = data_files[\"Observed\"]\n",
    "    mso_df = data_files[\"MSO\"]\n",
    "    wyt_df = data_files[\"WYT\"]\n",
    "    lstm_forecasts = data_files[\"Forecasts\"]\n",
    "    \n",
    "    # Get all available years from observed data\n",
    "    if observed_df.empty:\n",
    "        print(\"Error: Observed data is missing. Cannot calculate metrics.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    all_years = sorted(observed_df['Year'].unique())\n",
    "    \n",
    "    # Create a list to store all metrics rows\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Process each year\n",
    "    for year in all_years:\n",
    "        # Check if year has observed data for all 9 months\n",
    "        observed_data = observed_df[\n",
    "            (observed_df['Year'] == year) & \n",
    "            (observed_df['Month'].isin(range(1, 10)))\n",
    "        ]\n",
    "        \n",
    "        if len(observed_data) != 9:\n",
    "            print(f\"Skipping year {year}: Incomplete observed data ({len(observed_data)}/9 months)\")\n",
    "            continue\n",
    "        \n",
    "        # Get observed values for this year\n",
    "        observed_values = observed_data['F'].values\n",
    "        \n",
    "        # Get WYT for this year\n",
    "        wyt_value = \"Unknown\"\n",
    "        if not wyt_df.empty and 'year' in wyt_df.columns and 'wyt' in wyt_df.columns:\n",
    "            year_wyt = wyt_df[wyt_df['year'] == year]['wyt'].values\n",
    "            if len(year_wyt) > 0:\n",
    "                wyt_value = year_wyt[0]\n",
    "        \n",
    "        # Determine if year is in training or test set\n",
    "        is_training = not is_test_year(year)\n",
    "        period_label = \"Training\" if is_training else \"Test\"\n",
    "        \n",
    "        # Process MSO forecast if available\n",
    "        if not mso_df.empty and year in mso_df['Year'].values:\n",
    "            mso_forecast = mso_df[mso_df['Year'] == year].iloc[0]\n",
    "            \n",
    "            try:\n",
    "                mso_values = np.array([float(mso_forecast[month]) for month in months])\n",
    "                \n",
    "                # Calculate metrics\n",
    "                excel_r2 = calculate_excel_r2(observed_values, mso_values)\n",
    "                sklearn_r2 = r2_score(observed_values, mso_values)\n",
    "                mae = mean_absolute_error(observed_values, mso_values)\n",
    "                custom_loss = calculate_custom_loss(observed_values, mso_values)\n",
    "                percent_bias = calculate_percentage_bias(observed_values, mso_values)\n",
    "                \n",
    "                # Add to metrics list\n",
    "                all_metrics.append({\n",
    "                    'Year': year,\n",
    "                    'Model': 'MSO',\n",
    "                    'Period': period_label,\n",
    "                    'WYT': wyt_value,\n",
    "                    'Excel_R2': excel_r2,\n",
    "                    'Sklearn_R2': sklearn_r2,\n",
    "                    'MAE': mae,\n",
    "                    'Custom_Loss': custom_loss,\n",
    "                    'Percent_Bias': percent_bias\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating MSO metrics for year {year}: {str(e)}\")\n",
    "        \n",
    "        # Process each LSTM model\n",
    "        for model_name, lstm_df in lstm_forecasts.items():\n",
    "            if year not in lstm_df['Year'].values:\n",
    "                continue\n",
    "                \n",
    "            lstm_forecast = lstm_df[lstm_df['Year'] == year].iloc[0]\n",
    "            \n",
    "            # Check if F_month columns exist\n",
    "            f_columns = [f'F_{month}' for month in months]\n",
    "            if not all(col in lstm_forecast.index for col in f_columns):\n",
    "                print(f\"Skipping {model_name} for year {year}: Incomplete forecast data\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                lstm_values = np.array([float(lstm_forecast[f'F_{month}']) for month in months])\n",
    "                \n",
    "                # Calculate metrics\n",
    "                excel_r2 = calculate_excel_r2(observed_values, lstm_values)\n",
    "                sklearn_r2 = r2_score(observed_values, lstm_values)\n",
    "                mae = mean_absolute_error(observed_values, lstm_values)\n",
    "                custom_loss = calculate_custom_loss(observed_values, lstm_values)\n",
    "                percent_bias = calculate_percentage_bias(observed_values, lstm_values)\n",
    "                \n",
    "                # Add to metrics list\n",
    "                all_metrics.append({\n",
    "                    'Year': year,\n",
    "                    'Model': model_name,\n",
    "                    'Period': period_label,\n",
    "                    'WYT': wyt_value,\n",
    "                    'Excel_R2': excel_r2,\n",
    "                    'Sklearn_R2': sklearn_r2,\n",
    "                    'MAE': mae,\n",
    "                    'Custom_Loss': custom_loss,\n",
    "                    'Percent_Bias': percent_bias\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating {model_name} metrics for year {year}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data files...\")\n",
    "    data_files = load_data()\n",
    "    \n",
    "    print(\"\\nCalculating metrics for all years and models...\")\n",
    "    metrics_df = calculate_metrics_for_all_years(data_files)\n",
    "    \n",
    "    if metrics_df.empty:\n",
    "        print(\"Error: No metrics were calculated. Check that data files are available.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully calculated metrics for {len(metrics_df)} year/model combinations\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = 'all_metrics_summary.csv'\n",
    "    metrics_df.to_csv(output_file, index=False)\n",
    "    print(f\"Metrics saved to {output_file}\")\n",
    "    \n",
    "    # Generate summary statistics by model and period\n",
    "    try:\n",
    "        # Group by Model and Period\n",
    "        summary = metrics_df.groupby(['Model', 'Period']).agg({\n",
    "            'Excel_R2': ['mean', 'std', 'min', 'max'],\n",
    "            'Sklearn_R2': ['mean', 'std', 'min', 'max'],\n",
    "            'MAE': ['mean', 'std', 'min', 'max'],\n",
    "            'Custom_Loss': ['mean', 'std', 'min', 'max'],\n",
    "            'Percent_Bias': ['mean', 'std', 'min', 'max']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten multi-level columns\n",
    "        summary.columns = ['_'.join(col).strip('_') for col in summary.columns.values]\n",
    "        \n",
    "        summary_file = 'metrics_summary_by_model_period.csv'\n",
    "        summary.to_csv(summary_file, index=False)\n",
    "        print(f\"Summary statistics saved to {summary_file}\")\n",
    "        \n",
    "        # Generate additional summary by Water Year Type\n",
    "        wyt_summary = metrics_df.groupby(['Model', 'WYT']).agg({\n",
    "            'Excel_R2': ['mean', 'std'],\n",
    "            'MAE': ['mean', 'std'],\n",
    "            'Custom_Loss': ['mean', 'std']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten multi-level columns\n",
    "        wyt_summary.columns = ['_'.join(col).strip('_') for col in wyt_summary.columns.values]\n",
    "        \n",
    "        wyt_summary_file = 'metrics_summary_by_wyt.csv'\n",
    "        wyt_summary.to_csv(wyt_summary_file, index=False)\n",
    "        print(f\"WYT summary statistics saved to {wyt_summary_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary statistics: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DWR_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
