{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>P</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/1/1921</td>\n",
       "      <td>50.711506</td>\n",
       "      <td>9.680717</td>\n",
       "      <td>1.173617</td>\n",
       "      <td>81.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/1/1921</td>\n",
       "      <td>42.727805</td>\n",
       "      <td>6.427311</td>\n",
       "      <td>2.666764</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/1/1921</td>\n",
       "      <td>36.407265</td>\n",
       "      <td>3.427978</td>\n",
       "      <td>10.194451</td>\n",
       "      <td>194.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/1922</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.775463</td>\n",
       "      <td>2.969249</td>\n",
       "      <td>192.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/1/1922</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.077533</td>\n",
       "      <td>14.370800</td>\n",
       "      <td>422.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>5/1/2021</td>\n",
       "      <td>54.776440</td>\n",
       "      <td>11.438191</td>\n",
       "      <td>0.265424</td>\n",
       "      <td>136.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>6/1/2021</td>\n",
       "      <td>66.966054</td>\n",
       "      <td>19.161114</td>\n",
       "      <td>0.136572</td>\n",
       "      <td>88.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>7/1/2021</td>\n",
       "      <td>73.537018</td>\n",
       "      <td>24.017705</td>\n",
       "      <td>0.086182</td>\n",
       "      <td>70.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>8/1/2021</td>\n",
       "      <td>70.023603</td>\n",
       "      <td>21.625689</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>56.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>9/1/2021</td>\n",
       "      <td>64.739571</td>\n",
       "      <td>18.231172</td>\n",
       "      <td>0.582271</td>\n",
       "      <td>56.281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date          T          V          P        F\n",
       "0     10/1/1921  50.711506   9.680717   1.173617   81.200\n",
       "1     11/1/1921  42.727805   6.427311   2.666764  100.000\n",
       "2     12/1/1921  36.407265   3.427978  10.194451  194.000\n",
       "3      1/1/1922  32.000000   2.775463   2.969249  192.000\n",
       "4      2/1/1922  32.000000   2.077533  14.370800  422.000\n",
       "...         ...        ...        ...        ...      ...\n",
       "1195   5/1/2021  54.776440  11.438191   0.265424  136.858\n",
       "1196   6/1/2021  66.966054  19.161114   0.136572   88.619\n",
       "1197   7/1/2021  73.537018  24.017705   0.086182   70.112\n",
       "1198   8/1/2021  70.023603  21.625689   0.024613   56.391\n",
       "1199   9/1/2021  64.739571  18.231172   0.582271   56.281\n",
       "\n",
       "[1200 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "columns_to_analyze = ['Date','T', 'V', 'P', 'F']\n",
    "df = df[columns_to_analyze]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 1200\n",
      "Training dataset size (1943-2021): 945\n",
      "Test dataset size (1923-1942): 240\n",
      "Starting training process...\n",
      "WARNING:tensorflow:From c:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "\n",
      "Training model: lstm_h2_32_16_a0.1_b0.1_1943_2021\n",
      "Architecture: [32, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">153</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m4,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m153\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,025</span> (31.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,025\u001b[0m (31.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,025</span> (31.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,025\u001b[0m (31.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 100/5000 - Train Loss: 111465.2812 - Test Loss: 83630.3672\n",
      "Epoch 200/5000 - Train Loss: 83203.8828 - Test Loss: 101200.0156\n",
      "Epoch 300/5000 - Train Loss: 70841.6562 - Test Loss: 110538.4062\n",
      "Epoch 400/5000 - Train Loss: 58147.6719 - Test Loss: 104707.1172\n",
      "Epoch 500/5000 - Train Loss: 69494.2031 - Test Loss: 129351.0156\n",
      "Epoch 600/5000 - Train Loss: 49491.5977 - Test Loss: 126448.3438\n",
      "Epoch 700/5000 - Train Loss: 40558.8945 - Test Loss: 131068.9062\n",
      "Epoch 800/5000 - Train Loss: 37590.0312 - Test Loss: 134244.2656\n",
      "Epoch 900/5000 - Train Loss: 38138.4492 - Test Loss: 145591.6875\n",
      "Epoch 1000/5000 - Train Loss: 33566.5547 - Test Loss: 149449.1875\n",
      "Epoch 1100/5000 - Train Loss: 30338.3398 - Test Loss: 155379.4688\n",
      "Epoch 1200/5000 - Train Loss: 28242.7461 - Test Loss: 154576.5938\n",
      "Epoch 1300/5000 - Train Loss: 27615.6191 - Test Loss: 150224.4219\n",
      "Epoch 1400/5000 - Train Loss: 28563.2109 - Test Loss: 167964.2188\n",
      "Epoch 1500/5000 - Train Loss: 39347.8477 - Test Loss: 144514.1875\n",
      "Epoch 1600/5000 - Train Loss: 25429.7852 - Test Loss: 160079.5625\n",
      "Epoch 1700/5000 - Train Loss: 23064.9668 - Test Loss: 162124.9844\n",
      "Epoch 1800/5000 - Train Loss: 20885.3145 - Test Loss: 158218.6875\n",
      "Epoch 1900/5000 - Train Loss: 32517.3594 - Test Loss: 184140.6406\n",
      "Epoch 2000/5000 - Train Loss: 19603.3281 - Test Loss: 160657.4531\n",
      "Epoch 2100/5000 - Train Loss: 18743.8418 - Test Loss: 165324.0000\n",
      "Epoch 2200/5000 - Train Loss: 16885.2363 - Test Loss: 157500.5938\n",
      "Epoch 2300/5000 - Train Loss: 17904.9824 - Test Loss: 155950.5469\n",
      "Epoch 2400/5000 - Train Loss: 22162.3691 - Test Loss: 150304.9219\n",
      "Epoch 2500/5000 - Train Loss: 16831.8926 - Test Loss: 149018.5156\n",
      "Epoch 2600/5000 - Train Loss: 15629.5615 - Test Loss: 151709.4531\n",
      "Epoch 2700/5000 - Train Loss: 28624.5820 - Test Loss: 140584.3750\n",
      "Epoch 2800/5000 - Train Loss: 14227.9600 - Test Loss: 145748.7031\n",
      "Epoch 2900/5000 - Train Loss: 16059.3555 - Test Loss: 153155.8594\n",
      "Epoch 3000/5000 - Train Loss: 14379.8018 - Test Loss: 149679.2344\n",
      "Epoch 3100/5000 - Train Loss: 13356.2764 - Test Loss: 153937.6562\n",
      "Epoch 3200/5000 - Train Loss: 13393.6953 - Test Loss: 157253.2031\n",
      "Epoch 3300/5000 - Train Loss: 13669.3799 - Test Loss: 154362.2656\n",
      "Epoch 3400/5000 - Train Loss: 13239.6738 - Test Loss: 156956.9219\n",
      "Epoch 3500/5000 - Train Loss: 13255.8945 - Test Loss: 152735.2031\n",
      "Epoch 3600/5000 - Train Loss: 13792.5820 - Test Loss: 156644.2031\n",
      "Epoch 3700/5000 - Train Loss: 13894.0586 - Test Loss: 159588.5781\n",
      "Epoch 3800/5000 - Train Loss: 12870.9453 - Test Loss: 152006.8594\n",
      "Epoch 3900/5000 - Train Loss: 12754.1152 - Test Loss: 154928.8750\n",
      "Epoch 4000/5000 - Train Loss: 13080.2783 - Test Loss: 154547.3594\n",
      "Epoch 4100/5000 - Train Loss: 12909.8721 - Test Loss: 156369.4531\n",
      "Epoch 4200/5000 - Train Loss: 12863.1230 - Test Loss: 162259.2500\n",
      "Epoch 4300/5000 - Train Loss: 12578.2852 - Test Loss: 161026.2969\n",
      "Epoch 4400/5000 - Train Loss: 12649.7217 - Test Loss: 158352.6562\n",
      "Epoch 4500/5000 - Train Loss: 12818.5195 - Test Loss: 153092.3594\n",
      "Epoch 4600/5000 - Train Loss: 12714.5586 - Test Loss: 157161.2969\n",
      "Epoch 4700/5000 - Train Loss: 12559.0312 - Test Loss: 166495.6719\n",
      "Epoch 4800/5000 - Train Loss: 12671.0723 - Test Loss: 171501.9219\n",
      "Epoch 4900/5000 - Train Loss: 14028.3711 - Test Loss: 174032.6875\n",
      "Epoch 5000/5000 - Train Loss: 12864.5361 - Test Loss: 157546.7812\n",
      "\n",
      "Final training loss: 12864.5361\n",
      "Final test loss: 157546.7812\n",
      "\n",
      "Training and evaluation completed successfully!\n",
      "Train-test loss plot saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.activations import relu\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initial random seed setting\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Smoothness penalty coefficient\n",
    "beta = 0.1   # Monthly weight coefficient\n",
    "epochs = 5000\n",
    "input_window = 12\n",
    "output_window = 9\n",
    "n_features = 4  # PC1 to PC4\n",
    "batch_size = 32\n",
    "\n",
    "# Define LSTM architecture - using only one model with two hidden layers\n",
    "lstm_architecture = [32, 16]  # 2-layer\n",
    "\n",
    "# Custom loss function\n",
    "def create_custom_flow_loss(alpha, beta):\n",
    "    def custom_flow_loss(y_true, y_pred):\n",
    "        month_weights = tf.constant([3, 3, 3, 2, 2, 1, 1, 1, 1], dtype=tf.float32)\n",
    "        mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "        \n",
    "        # Smoothness penalty\n",
    "        smoothness_penalty = tf.reduce_mean(tf.square(y_pred[:, 1:] - y_pred[:, :-1]))\n",
    "        \n",
    "        # Monthly weighted loss\n",
    "        monthly_weight_loss = tf.reduce_mean(month_weights * tf.square(y_true - y_pred))\n",
    "        \n",
    "        return mse_loss + alpha * smoothness_penalty + beta * monthly_weight_loss\n",
    "    return custom_flow_loss\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=4)\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        features = df[['T', 'V', 'P', 'F']].values\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        pca_features = self.pca.fit_transform(features_scaled)\n",
    "        return pca_features\n",
    "    \n",
    "    def transform(self, df):\n",
    "        features = df[['T', 'V', 'P', 'F']].values\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        pca_features = self.pca.transform(features_scaled)\n",
    "        return pca_features\n",
    "    \n",
    "    def save(self, filename):\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca\n",
    "        }\n",
    "        joblib.dump(preprocessor_dict, filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        preprocessor = cls()\n",
    "        loaded_dict = joblib.load(filename)\n",
    "        preprocessor.scaler = loaded_dict['scaler']\n",
    "        preprocessor.pca = loaded_dict['pca']\n",
    "        return preprocessor\n",
    "\n",
    "def prepare_sequences(features, target):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - input_window - output_window + 1):\n",
    "        X.append(features[i:(i + input_window)])\n",
    "        y.append(target[i + input_window:i + input_window + output_window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(architecture):\n",
    "    # Ensure clean state for model creation\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(architecture[0], activation='relu', return_sequences=True if len(architecture) > 1 else False,\n",
    "                  input_shape=(input_window, n_features)))\n",
    "    \n",
    "    # Middle LSTM layers\n",
    "    for i in range(1, len(architecture) - 1):\n",
    "        model.add(LSTM(architecture[i], activation='relu', return_sequences=True))\n",
    "    \n",
    "    # Last LSTM layer\n",
    "    if len(architecture) > 1:\n",
    "        model.add(LSTM(architecture[-1], activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(output_window, activation='relu'))\n",
    "    \n",
    "    custom_loss = create_custom_flow_loss(alpha, beta)\n",
    "    model.compile(optimizer='adam', loss=custom_loss)\n",
    "    return model\n",
    "\n",
    "def plot_train_test_losses(train_history, test_losses, save_path='train_test_losses.png'):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.plot(train_history['loss'], label='Training Loss', color='blue', alpha=0.8)\n",
    "    \n",
    "    # Plot test loss for each epoch\n",
    "    plt.plot(test_losses, label='Test Loss', color='red', alpha=0.8)\n",
    "    \n",
    "    plt.title('Training and Test Loss Comparison (1943-2021 vs 1923-1942)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def get_model_name(architecture, alpha, beta):\n",
    "    # Create name based on number of hidden layers and neurons\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"lstm_{arch_str}_a{alpha}_b{beta}_1943_2021\"\n",
    "\n",
    "def evaluate_on_test_data(model, X_test, y_test, custom_loss):\n",
    "    \"\"\"Evaluate the model on test data and return the loss.\"\"\"\n",
    "    return model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "def train_and_evaluate_model(df_train, df_test):\n",
    "    # Initialize and fit preprocessor on training data\n",
    "    preprocessor = DataPreprocessor()\n",
    "    pca_features_train = preprocessor.fit_transform(df_train)\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor.save('preprocessor_1943_2021.joblib')\n",
    "    \n",
    "    # Transform test data using the same preprocessor\n",
    "    pca_features_test = preprocessor.transform(df_test)\n",
    "    \n",
    "    # Prepare sequences for training and testing\n",
    "    X_train, y_train = prepare_sequences(pca_features_train, df_train['F'].values)\n",
    "    X_test, y_test = prepare_sequences(pca_features_test, df_test['F'].values)\n",
    "    \n",
    "    # Clear everything for clean start\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Reset random seeds for reproducibility\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    \n",
    "    # Get model name\n",
    "    model_name = get_model_name(lstm_architecture, alpha, beta)\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    print(f\"Architecture: {lstm_architecture}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(lstm_architecture)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Create custom loss function for evaluation\n",
    "    custom_loss = create_custom_flow_loss(alpha, beta)\n",
    "    \n",
    "    # Store test losses for each epoch\n",
    "    test_losses = []\n",
    "    \n",
    "    # Custom callback to evaluate on test data after each epoch\n",
    "    class TestEvaluationCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            test_loss = evaluate_on_test_data(self.model, X_test, y_test, custom_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {logs['loss']:.4f} - Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Train model with callback\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[TestEvaluationCallback()]\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'{model_name}.keras')\n",
    "    \n",
    "    # Plot train and test losses\n",
    "    plot_train_test_losses(history.history, test_losses, save_path=f'{model_name}_train_test_losses.png')\n",
    "    \n",
    "    # Print final losses\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_test_loss = test_losses[-1]\n",
    "    print(f\"\\nFinal training loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final test loss: {final_test_loss:.4f}\")\n",
    "    \n",
    "    return history.history, test_losses, preprocessor\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df_full = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        \n",
    "        # Convert date column to datetime if it's not already\n",
    "        if 'Date' in df_full.columns:\n",
    "            df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
    "            \n",
    "            # Define train period (1943-2021) and test period (1923-1942)\n",
    "            train_start_date = pd.to_datetime('1943-01-01')\n",
    "            test_start_date = pd.to_datetime('1923-01-01')\n",
    "            test_end_date = pd.to_datetime('1942-12-31')\n",
    "            \n",
    "            # Filter data for training and testing\n",
    "            df_train = df_full[df_full['Date'] >= train_start_date]\n",
    "            df_test = df_full[(df_full['Date'] >= test_start_date) & (df_full['Date'] <= test_end_date)]\n",
    "            \n",
    "            print(f\"Full dataset size: {len(df_full)}\")\n",
    "            print(f\"Training dataset size (1943-2021): {len(df_train)}\")\n",
    "            print(f\"Test dataset size (1923-1942): {len(df_test)}\")\n",
    "            \n",
    "            # Train model and evaluate on test data\n",
    "            print(\"Starting training process...\")\n",
    "            train_history, test_losses, preprocessor = train_and_evaluate_model(df_train, df_test)\n",
    "            print(\"\\nTraining and evaluation completed successfully!\")\n",
    "            \n",
    "            # Save final plot\n",
    "            print(\"Train-test loss plot saved.\")\n",
    "        else:\n",
    "            print(\"Error: Dataset does not contain a 'Date' column. Please adjust the code to match your date column name.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DWR_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
