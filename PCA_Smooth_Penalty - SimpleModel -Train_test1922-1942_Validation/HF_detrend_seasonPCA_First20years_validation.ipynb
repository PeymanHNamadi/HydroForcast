{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying enhanced preprocessing with detrending and feature engineering...\n",
      "Explained variance by 5 PCs: [0.44724241 0.236193   0.15201838 0.09396612 0.05300994]\n",
      "Total variance explained: 0.9824\n",
      "Preprocessor saved to enhanced_preprocessor_pc5.joblib\n",
      "Full dataset size: 1200\n",
      "Training dataset size (1963-2021): 705\n",
      "Validation dataset size (1943-1962): 240\n",
      "Test dataset size (1923-1942): 240\n",
      "Training sequences: (685, 12, 5)\n",
      "Validation sequences: (220, 12, 5)\n",
      "Test sequences: (220, 12, 5)\n",
      "\n",
      "Training model: enhanced_lstm_h2_32_16_a0.1_b0.1_pc5\n",
      "Architecture: [32, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">153</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m4,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m3,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m153\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,153</span> (31.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,153\u001b[0m (31.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,153</span> (31.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,153\u001b[0m (31.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 100 - Train: 103949.9453, Val: 408276.1875, Test: 1429634.3750\n",
      "Epoch 200 - Train: 64954.0664, Val: 501435.2812, Test: 2227711.2500\n",
      "Epoch 234: early stopping\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Model saved as enhanced_lstm_h2_32_16_a0.1_b0.1_pc5.keras\n",
      "\n",
      "Model training stopped after 234 epochs\n",
      "Early stopping activated (patience=200)\n",
      "Best epoch: 34\n",
      "Final training loss: 62072.8828\n",
      "Final validation loss: 410917.6875\n",
      "Final test loss: 1700400.1250\n",
      "Best validation loss: 176773.7812\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Training and evaluation completed successfully!\n",
      "Results and plots saved with prefix: enhanced_lstm_h2_32_16_a0.1_b0.1_pc5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initial random seed setting\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Smoothness penalty coefficient\n",
    "beta = 0.1   # Monthly weight coefficient\n",
    "epochs = 5000\n",
    "input_window = 12\n",
    "output_window = 9\n",
    "n_features = 5  # PC1 to PC5 (updated from 4 to 5)\n",
    "batch_size = 32\n",
    "\n",
    "# Define LSTM architecture - using only one model with two hidden layers\n",
    "lstm_architecture = [32, 16]  # 2-layer\n",
    "\n",
    "# Custom loss function\n",
    "def create_custom_flow_loss(alpha, beta):\n",
    "    def custom_flow_loss(y_true, y_pred):\n",
    "        month_weights = tf.constant([3, 3, 3, 2, 2, 1, 1, 1, 1], dtype=tf.float32)\n",
    "        mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "        \n",
    "        # Smoothness penalty\n",
    "        smoothness_penalty = tf.reduce_mean(tf.square(y_pred[:, 1:] - y_pred[:, :-1]))\n",
    "        \n",
    "        # Monthly weighted loss\n",
    "        monthly_weight_loss = tf.reduce_mean(month_weights * tf.square(y_true - y_pred))\n",
    "        \n",
    "        return mse_loss + alpha * smoothness_penalty + beta * monthly_weight_loss\n",
    "    return custom_flow_loss\n",
    "\n",
    "class EnhancedDataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=5)  # Using 5 components as requested\n",
    "        self.detrend_params = {}  # Will store detrending parameters\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        # Step 1: Detrend the significant variables (T and V)\n",
    "        df_detrended = df.copy()\n",
    "        features_to_detrend = ['T', 'V']  # Focus on statistically significant variables\n",
    "        \n",
    "        for col in features_to_detrend:\n",
    "            values = df[col].values\n",
    "            x = np.arange(len(values))\n",
    "            slope, intercept = np.polyfit(x, values, 1)\n",
    "            trend = slope * x + intercept\n",
    "            \n",
    "            # Store detrending parameters for future transformations\n",
    "            self.detrend_params[col] = {'slope': slope, 'intercept': intercept}\n",
    "            \n",
    "            # Replace original with detrended and add trend as new feature\n",
    "            df_detrended[col] = values - trend\n",
    "            df_detrended[f'{col}_trend'] = trend\n",
    "        \n",
    "        # Step 2: Engineer additional features\n",
    "        # Add key interaction features\n",
    "        df_detrended['T_V_ratio'] = df['T'] / df['V']\n",
    "        df_detrended['seasonal_T'] = self._extract_seasonal_component(df, 'T')\n",
    "        df_detrended['seasonal_P'] = self._extract_seasonal_component(df, 'P')\n",
    "        \n",
    "        # Step 3: Select final feature set for PCA\n",
    "        feature_columns = ['T', 'V', 'P', 'F', 'T_trend', 'V_trend', \n",
    "                          'T_V_ratio', 'seasonal_T', 'seasonal_P']\n",
    "        features = df_detrended[feature_columns].values\n",
    "        \n",
    "        # Step 4: Apply scaling and PCA\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        pca_features = self.pca.fit_transform(features_scaled)\n",
    "        \n",
    "        # Print explained variance to understand component importance\n",
    "        explained_variance = self.pca.explained_variance_ratio_\n",
    "        print(f\"Explained variance by 5 PCs: {explained_variance}\")\n",
    "        print(f\"Total variance explained: {sum(explained_variance):.4f}\")\n",
    "        \n",
    "        return pca_features\n",
    "    \n",
    "    def _extract_seasonal_component(self, df, column):\n",
    "        # Extract monthly seasonal components\n",
    "        series = df[column]\n",
    "        grouped = series.groupby(pd.DatetimeIndex(df['Date']).month)\n",
    "        monthly_means = df[column].copy()\n",
    "        \n",
    "        for month, group in grouped:\n",
    "            mask = pd.DatetimeIndex(df['Date']).month == month\n",
    "            monthly_means.loc[mask] = group.mean()\n",
    "            \n",
    "        return series - monthly_means\n",
    "        \n",
    "    def transform(self, df):\n",
    "        # Apply same transformations to new data\n",
    "        df_detrended = df.copy()\n",
    "        \n",
    "        # Apply stored detrending parameters\n",
    "        for col, params in self.detrend_params.items():\n",
    "            x = np.arange(len(df))\n",
    "            trend = params['slope'] * x + params['intercept']\n",
    "            df_detrended[col] = df[col] - trend\n",
    "            df_detrended[f'{col}_trend'] = trend\n",
    "        \n",
    "        # Create same engineered features\n",
    "        df_detrended['T_V_ratio'] = df['T'] / df['V']\n",
    "        df_detrended['seasonal_T'] = self._extract_seasonal_component(df, 'T')\n",
    "        df_detrended['seasonal_P'] = self._extract_seasonal_component(df, 'P')\n",
    "        \n",
    "        # Select same feature set and apply transformations\n",
    "        feature_columns = ['T', 'V', 'P', 'F', 'T_trend', 'V_trend', \n",
    "                          'T_V_ratio', 'seasonal_T', 'seasonal_P']\n",
    "        features = df_detrended[feature_columns].values\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        pca_features = self.pca.transform(features_scaled)\n",
    "        \n",
    "        return pca_features\n",
    "    \n",
    "    def save(self, filename):\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca,\n",
    "            'detrend_params': self.detrend_params,\n",
    "        }\n",
    "        joblib.dump(preprocessor_dict, filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        preprocessor = cls()\n",
    "        loaded_dict = joblib.load(filename)\n",
    "        preprocessor.scaler = loaded_dict['scaler']\n",
    "        preprocessor.pca = loaded_dict['pca']\n",
    "        preprocessor.detrend_params = loaded_dict['detrend_params']\n",
    "        return preprocessor\n",
    "\n",
    "def prepare_sequences(features, target):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - input_window - output_window + 1):\n",
    "        X.append(features[i:(i + input_window)])\n",
    "        y.append(target[i + input_window:i + input_window + output_window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(architecture):\n",
    "    # Ensure clean state for model creation\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(architecture[0], activation='relu', return_sequences=True if len(architecture) > 1 else False,\n",
    "                  input_shape=(input_window, n_features)))\n",
    "    \n",
    "    # Middle LSTM layers\n",
    "    for i in range(1, len(architecture) - 1):\n",
    "        model.add(LSTM(architecture[i], activation='relu', return_sequences=True))\n",
    "    \n",
    "    # Last LSTM layer\n",
    "    if len(architecture) > 1:\n",
    "        model.add(LSTM(architecture[-1], activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(output_window, activation='relu'))\n",
    "    \n",
    "    custom_loss = create_custom_flow_loss(alpha, beta)\n",
    "    model.compile(optimizer='adam', loss=custom_loss)\n",
    "    return model\n",
    "\n",
    "def plot_train_test_losses(train_history, val_history, test_losses, save_path='train_val_test_losses.png'):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.plot(train_history, label='Training Loss', color='blue', alpha=0.8)\n",
    "    \n",
    "    # Plot validation loss\n",
    "    plt.plot(val_history, label='Validation Loss (1943-1962)', color='green', alpha=0.8)\n",
    "    \n",
    "    # Plot test loss for each epoch\n",
    "    plt.plot(test_losses, label='Test Loss (1923-1942)', color='red', alpha=0.8)\n",
    "    \n",
    "    plt.title('Training, Validation and Test Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def get_model_name(architecture, alpha, beta):\n",
    "    # Create name based on number of hidden layers and neurons\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"enhanced_lstm_{arch_str}_a{alpha}_b{beta}_pc5\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df_full = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
    "        \n",
    "        # Preprocess ALL data at once with enhanced preprocessing\n",
    "        print(\"Applying enhanced preprocessing with detrending and feature engineering...\")\n",
    "        preprocessor = EnhancedDataPreprocessor()\n",
    "        all_features = preprocessor.fit_transform(df_full)\n",
    "        \n",
    "        # Save preprocessor for future use\n",
    "        preprocessor.save('enhanced_preprocessor_pc5.joblib')\n",
    "        print(\"Preprocessor saved to enhanced_preprocessor_pc5.joblib\")\n",
    "        \n",
    "        # Define date ranges for splits\n",
    "        test_start_date = pd.to_datetime('1923-01-01')\n",
    "        test_end_date = pd.to_datetime('1942-12-31')\n",
    "        train_start_date = pd.to_datetime('1963-01-01')\n",
    "        validation_start_date = pd.to_datetime('1943-01-01')\n",
    "        validation_end_date = pd.to_datetime('1962-12-31')\n",
    "        \n",
    "        # Create masks for each subset\n",
    "        test_mask = (df_full['Date'] >= test_start_date) & (df_full['Date'] <= test_end_date)\n",
    "        validation_mask = (df_full['Date'] >= validation_start_date) & (df_full['Date'] <= validation_end_date)\n",
    "        train_mask = df_full['Date'] >= train_start_date\n",
    "        \n",
    "        # Print dataset sizes\n",
    "        print(f\"Full dataset size: {len(df_full)}\")\n",
    "        print(f\"Training dataset size (1963-2021): {sum(train_mask)}\")\n",
    "        print(f\"Validation dataset size (1943-1962): {sum(validation_mask)}\")\n",
    "        print(f\"Test dataset size (1923-1942): {sum(test_mask)}\")\n",
    "        \n",
    "        # Prepare sequences for each subset\n",
    "        X_train, y_train = prepare_sequences(\n",
    "            all_features[train_mask], \n",
    "            df_full.loc[train_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        X_val, y_val = prepare_sequences(\n",
    "            all_features[validation_mask], \n",
    "            df_full.loc[validation_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        X_test, y_test = prepare_sequences(\n",
    "            all_features[test_mask], \n",
    "            df_full.loc[test_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        print(f\"Training sequences: {X_train.shape}\")\n",
    "        print(f\"Validation sequences: {X_val.shape}\")\n",
    "        print(f\"Test sequences: {X_test.shape}\")\n",
    "        \n",
    "        # Clear everything for clean start\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Reset random seeds for reproducibility\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "        \n",
    "        # Get model name\n",
    "        model_name = get_model_name(lstm_architecture, alpha, beta)\n",
    "        print(f\"\\nTraining model: {model_name}\")\n",
    "        print(f\"Architecture: {lstm_architecture}\")\n",
    "        \n",
    "        # Build model\n",
    "        model = build_model(lstm_architecture)\n",
    "        print(model.summary())\n",
    "        \n",
    "        # Create custom loss function for evaluation\n",
    "        custom_loss = create_custom_flow_loss(alpha, beta)\n",
    "        \n",
    "        # Store losses for tracking\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # Set up callbacks\n",
    "        # 1. Early stopping based on validation loss\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=200,  # Number of epochs with no improvement to wait before stopping\n",
    "            restore_best_weights=True,  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # 2. Custom callback to evaluate on test data after each epoch\n",
    "        class TestEvaluationCallback(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                # Store training and validation losses from logs\n",
    "                train_losses.append(logs['loss'])\n",
    "                val_losses.append(logs['val_loss'])\n",
    "                \n",
    "                # Evaluate on test data\n",
    "                test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "                    print(f\"Epoch {epoch + 1} - Train: {logs['loss']:.4f}, Val: {logs['val_loss']:.4f}, Test: {test_loss:.4f}\")\n",
    "        \n",
    "        # Train model with proper validation split and callbacks\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),  # Properly use validation data\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=[\n",
    "                early_stopping,\n",
    "                TestEvaluationCallback()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        model.save(f'{model_name}.keras')\n",
    "        print(f\"Model saved as {model_name}.keras\")\n",
    "        \n",
    "        # Plot train, validation and test losses\n",
    "        plot_train_test_losses(\n",
    "            train_losses, \n",
    "            val_losses, \n",
    "            test_losses, \n",
    "            save_path=f'{model_name}_train_val_test_losses.png'\n",
    "        )\n",
    "        \n",
    "        # Print final losses and training information\n",
    "        final_train_loss = train_losses[-1]\n",
    "        final_val_loss = val_losses[-1]\n",
    "        final_test_loss = test_losses[-1]\n",
    "        \n",
    "        print(f\"\\nModel training stopped after {len(train_losses)} epochs\")\n",
    "        if len(train_losses) < epochs:\n",
    "            print(f\"Early stopping activated (patience={early_stopping.patience})\")\n",
    "            \n",
    "        print(f\"Best epoch: {np.argmin(val_losses) + 1}\")\n",
    "        print(f\"Final training loss: {final_train_loss:.4f}\")\n",
    "        print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "        print(f\"Final test loss: {final_test_loss:.4f}\")\n",
    "        print(f\"Best validation loss: {min(val_losses):.4f}\")\n",
    "        \n",
    "        # Generate predictions for test data\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Plot actual vs predicted for the first few test samples\n",
    "        n_samples = min(5, len(predictions))\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(n_samples, 1, i+1)\n",
    "            plt.plot(y_test[i], 'b-', label='Actual')\n",
    "            plt.plot(predictions[i], 'r--', label='Predicted')\n",
    "            plt.legend()\n",
    "            plt.title(f'Test Sample {i+1}')\n",
    "            plt.ylabel('Flow')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name}_predictions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Training and evaluation completed successfully!\")\n",
    "        print(f\"Results and plots saved with prefix: {model_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 1200\n",
      "Training dataset size (1963-2021): 705\n",
      "Validation dataset size (1943-1962): 240\n",
      "Test dataset size (1923-1942): 240\n",
      "\n",
      "=== Scenario 1: Basic Preprocessing (PCA with 4 components) ===\n",
      "Basic PCA - Explained variance by 4 PCs: [0.71998728 0.19835198 0.07562216 0.00603858]\n",
      "Basic PCA - Total variance explained: 1.0000\n",
      "\n",
      "Training model: lstm_basic_h2_32_16_a0.1_b0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - Train: 129918.3906, Val: 102819.2812, Test: 100848.6250\n",
      "Epoch 200 - Train: 91359.2734, Val: 138433.0312, Test: 117339.0703\n",
      "Epoch 245: early stopping\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "\n",
      "Model training stopped after 245 epochs\n",
      "Early stopping activated (patience=200)\n",
      "Best epoch: 45\n",
      "Final training loss: 81492.0234\n",
      "Final validation loss: 151824.1562\n",
      "Final test loss: 122227.5391\n",
      "Best validation loss: 97051.4766\n",
      "\n",
      "=== Scenario 2: Enhanced Preprocessing (Detrending + Feature Engineering + PCA) ===\n",
      "Enhanced PCA - Explained variance by 5 PCs: [0.44724241 0.236193   0.15201838 0.09396612 0.05300994]\n",
      "Enhanced PCA - Total variance explained: 0.9824\n",
      "\n",
      "Training model: lstm_enhanced_h2_32_16_a0.1_b0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peymanhn\\AppData\\Local\\anaconda3\\envs\\ML_DWR_2025\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - Train: 92307.0391, Val: 363780.0000, Test: 2022806.5000\n",
      "Epoch 200 - Train: 65822.2891, Val: 372287.4062, Test: 1776052.6250\n",
      "Epoch 278: early stopping\n",
      "Restoring model weights from the end of the best epoch: 78.\n",
      "\n",
      "Model training stopped after 278 epochs\n",
      "Early stopping activated (patience=200)\n",
      "Best epoch: 78\n",
      "Final training loss: 56768.9336\n",
      "Final validation loss: 374259.5625\n",
      "Final test loss: 1487142.1250\n",
      "Best validation loss: 178161.4688\n",
      "\n",
      "=== Comparing Preprocessing Approaches ===\n",
      "Comparison plot saved to preprocessing_comparison_losses.png\n",
      "\n",
      "Best validation loss - Basic: 97051.4766, Enhanced: 178161.4688\n",
      "Validation improvement: -83.57%\n",
      "Best test loss - Basic: 96535.7734, Enhanced: 205529.6719\n",
      "Test improvement: -112.91%\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002263C58EF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002263C58E310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Comparison predictions plot saved to preprocessing_comparison_predictions.png\n",
      "\n",
      "Preprocessing comparison analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Initial random seed setting\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Smoothness penalty coefficient\n",
    "beta = 0.1   # Monthly weight coefficient\n",
    "epochs = 5000\n",
    "input_window = 12\n",
    "output_window = 9\n",
    "batch_size = 32\n",
    "\n",
    "# Define LSTM architecture - using only one model with two hidden layers\n",
    "lstm_architecture = [32, 16]  # 2-layer\n",
    "\n",
    "# Custom loss function\n",
    "def create_custom_flow_loss(alpha, beta):\n",
    "    def custom_flow_loss(y_true, y_pred):\n",
    "        month_weights = tf.constant([3, 3, 3, 2, 2, 1, 1, 1, 1], dtype=tf.float32)\n",
    "        mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "        \n",
    "        # Smoothness penalty\n",
    "        smoothness_penalty = tf.reduce_mean(tf.square(y_pred[:, 1:] - y_pred[:, :-1]))\n",
    "        \n",
    "        # Monthly weighted loss\n",
    "        monthly_weight_loss = tf.reduce_mean(month_weights * tf.square(y_true - y_pred))\n",
    "        \n",
    "        return mse_loss + alpha * smoothness_penalty + beta * monthly_weight_loss\n",
    "    return custom_flow_loss\n",
    "\n",
    "class BasicDataPreprocessor:\n",
    "    \"\"\"Original preprocessing approach: Just PCA with 4 PCs\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=4)  # Original: 4 components\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        features = df[['T', 'V', 'P', 'F']].values\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        pca_features = self.pca.fit_transform(features_scaled)\n",
    "        \n",
    "        # Print explained variance\n",
    "        explained_variance = self.pca.explained_variance_ratio_\n",
    "        print(f\"Basic PCA - Explained variance by 4 PCs: {explained_variance}\")\n",
    "        print(f\"Basic PCA - Total variance explained: {sum(explained_variance):.4f}\")\n",
    "        \n",
    "        return pca_features\n",
    "    \n",
    "    def transform(self, df):\n",
    "        features = df[['T', 'V', 'P', 'F']].values\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        pca_features = self.pca.transform(features_scaled)\n",
    "        return pca_features\n",
    "    \n",
    "    def save(self, filename):\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca\n",
    "        }\n",
    "        joblib.dump(preprocessor_dict, filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        preprocessor = cls()\n",
    "        loaded_dict = joblib.load(filename)\n",
    "        preprocessor.scaler = loaded_dict['scaler']\n",
    "        preprocessor.pca = loaded_dict['pca']\n",
    "        return preprocessor\n",
    "\n",
    "class EnhancedDataPreprocessor:\n",
    "    \"\"\"Enhanced preprocessing approach: Detrending + Feature Engineering + PCA with 5 PCs\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=5)  # Enhanced: 5 components\n",
    "        self.detrend_params = {}  # Will store detrending parameters\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        # Step 1: Detrend the significant variables (T and V)\n",
    "        df_detrended = df.copy()\n",
    "        features_to_detrend = ['T', 'V']  # Focus on statistically significant variables\n",
    "        \n",
    "        for col in features_to_detrend:\n",
    "            values = df[col].values\n",
    "            x = np.arange(len(values))\n",
    "            slope, intercept = np.polyfit(x, values, 1)\n",
    "            trend = slope * x + intercept\n",
    "            \n",
    "            # Store detrending parameters for future transformations\n",
    "            self.detrend_params[col] = {'slope': slope, 'intercept': intercept}\n",
    "            \n",
    "            # Replace original with detrended and add trend as new feature\n",
    "            df_detrended[col] = values - trend\n",
    "            df_detrended[f'{col}_trend'] = trend\n",
    "        \n",
    "        # Step 2: Engineer additional features\n",
    "        # Add key interaction features\n",
    "        df_detrended['T_V_ratio'] = df['T'] / df['V']\n",
    "        df_detrended['seasonal_T'] = self._extract_seasonal_component(df, 'T')\n",
    "        df_detrended['seasonal_P'] = self._extract_seasonal_component(df, 'P')\n",
    "        \n",
    "        # Step 3: Select final feature set for PCA\n",
    "        feature_columns = ['T', 'V', 'P', 'F', 'T_trend', 'V_trend', \n",
    "                          'T_V_ratio', 'seasonal_T', 'seasonal_P']\n",
    "        features = df_detrended[feature_columns].values\n",
    "        \n",
    "        # Step 4: Apply scaling and PCA\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "        pca_features = self.pca.fit_transform(features_scaled)\n",
    "        \n",
    "        # Print explained variance\n",
    "        explained_variance = self.pca.explained_variance_ratio_\n",
    "        print(f\"Enhanced PCA - Explained variance by 5 PCs: {explained_variance}\")\n",
    "        print(f\"Enhanced PCA - Total variance explained: {sum(explained_variance):.4f}\")\n",
    "        \n",
    "        return pca_features\n",
    "    \n",
    "    def _extract_seasonal_component(self, df, column):\n",
    "        # Extract monthly seasonal components\n",
    "        series = df[column]\n",
    "        grouped = series.groupby(pd.DatetimeIndex(df['Date']).month)\n",
    "        monthly_means = df[column].copy()\n",
    "        \n",
    "        for month, group in grouped:\n",
    "            mask = pd.DatetimeIndex(df['Date']).month == month\n",
    "            monthly_means.loc[mask] = group.mean()\n",
    "            \n",
    "        return series - monthly_means\n",
    "        \n",
    "    def transform(self, df):\n",
    "        # Apply same transformations to new data\n",
    "        df_detrended = df.copy()\n",
    "        \n",
    "        # Apply stored detrending parameters\n",
    "        for col, params in self.detrend_params.items():\n",
    "            x = np.arange(len(df))\n",
    "            trend = params['slope'] * x + params['intercept']\n",
    "            df_detrended[col] = df[col] - trend\n",
    "            df_detrended[f'{col}_trend'] = trend\n",
    "        \n",
    "        # Create same engineered features\n",
    "        df_detrended['T_V_ratio'] = df['T'] / df['V']\n",
    "        df_detrended['seasonal_T'] = self._extract_seasonal_component(df, 'T')\n",
    "        df_detrended['seasonal_P'] = self._extract_seasonal_component(df, 'P')\n",
    "        \n",
    "        # Select same feature set and apply transformations\n",
    "        feature_columns = ['T', 'V', 'P', 'F', 'T_trend', 'V_trend', \n",
    "                          'T_V_ratio', 'seasonal_T', 'seasonal_P']\n",
    "        features = df_detrended[feature_columns].values\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        pca_features = self.pca.transform(features_scaled)\n",
    "        \n",
    "        return pca_features\n",
    "    \n",
    "    def save(self, filename):\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca,\n",
    "            'detrend_params': self.detrend_params,\n",
    "        }\n",
    "        joblib.dump(preprocessor_dict, filename)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        preprocessor = cls()\n",
    "        loaded_dict = joblib.load(filename)\n",
    "        preprocessor.scaler = loaded_dict['scaler']\n",
    "        preprocessor.pca = loaded_dict['pca']\n",
    "        preprocessor.detrend_params = loaded_dict['detrend_params']\n",
    "        return preprocessor\n",
    "\n",
    "def prepare_sequences(features, target):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - input_window - output_window + 1):\n",
    "        X.append(features[i:(i + input_window)])\n",
    "        y.append(target[i + input_window:i + input_window + output_window])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(architecture, n_features):\n",
    "    # Ensure clean state for model creation\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(architecture[0], activation='relu', return_sequences=True if len(architecture) > 1 else False,\n",
    "                  input_shape=(input_window, n_features)))\n",
    "    \n",
    "    # Middle LSTM layers\n",
    "    for i in range(1, len(architecture) - 1):\n",
    "        model.add(LSTM(architecture[i], activation='relu', return_sequences=True))\n",
    "    \n",
    "    # Last LSTM layer\n",
    "    if len(architecture) > 1:\n",
    "        model.add(LSTM(architecture[-1], activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(output_window, activation='relu'))\n",
    "    \n",
    "    custom_loss = create_custom_flow_loss(alpha, beta)\n",
    "    model.compile(optimizer='adam', loss=custom_loss)\n",
    "    return model\n",
    "\n",
    "def plot_comparison_losses(basic_losses, enhanced_losses, save_path='preprocessing_comparison.png'):\n",
    "    \"\"\"Plot losses from both preprocessing approaches on the same graph\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Extract losses\n",
    "    basic_train, basic_val, basic_test = basic_losses\n",
    "    enhanced_train, enhanced_val, enhanced_test = enhanced_losses\n",
    "    \n",
    "    # Plot training losses\n",
    "    plt.plot(basic_train, color='blue', linestyle='-', alpha=0.7, label='Basic - Training')\n",
    "    plt.plot(enhanced_train, color='blue', linestyle='--', alpha=0.7, label='Enhanced - Training')\n",
    "    \n",
    "    # Plot validation losses\n",
    "    plt.plot(basic_val, color='green', linestyle='-', alpha=0.7, label='Basic - Validation')\n",
    "    plt.plot(enhanced_val, color='green', linestyle='--', alpha=0.7, label='Enhanced - Validation')\n",
    "    \n",
    "    # Plot test losses\n",
    "    plt.plot(basic_test, color='red', linestyle='-', alpha=0.7, label='Basic - Test')\n",
    "    plt.plot(enhanced_test, color='red', linestyle='--', alpha=0.7, label='Enhanced - Test')\n",
    "    \n",
    "    # Add custom legend with color groups\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label='Training Loss'),\n",
    "        Line2D([0], [0], color='green', lw=2, label='Validation Loss'),\n",
    "        Line2D([0], [0], color='red', lw=2, label='Test Loss'),\n",
    "        Line2D([0], [0], color='black', linestyle='-', lw=2, label='Basic Preprocessing'),\n",
    "        Line2D([0], [0], color='black', linestyle='--', lw=2, label='Enhanced Preprocessing')\n",
    "    ]\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.title('Comparison of Basic vs Enhanced Preprocessing')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def get_model_name(architecture, alpha, beta, approach):\n",
    "    # Create name based on number of hidden layers and neurons\n",
    "    arch_str = f\"h{len(architecture)}_\" + \"_\".join(map(str, architecture))\n",
    "    return f\"lstm_{approach}_{arch_str}_a{alpha}_b{beta}\"\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, X_test, y_test, n_features, approach):\n",
    "    \"\"\"Train model with early stopping and return loss histories\"\"\"\n",
    "    # Clear everything for clean start\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Reset random seeds for reproducibility\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    \n",
    "    # Get model name\n",
    "    model_name = get_model_name(lstm_architecture, alpha, beta, approach)\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(lstm_architecture, n_features)\n",
    "    \n",
    "    # Set up callbacks and loss tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=200,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Callback to track test loss\n",
    "    class TestEvaluationCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            # Store training and validation losses from logs\n",
    "            train_losses.append(logs['loss'])\n",
    "            val_losses.append(logs['val_loss'])\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "                print(f\"Epoch {epoch + 1} - Train: {logs['loss']:.4f}, Val: {logs['val_loss']:.4f}, Test: {test_loss:.4f}\")\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            early_stopping,\n",
    "            TestEvaluationCallback()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'{model_name}.keras')\n",
    "    \n",
    "    # Print final performance\n",
    "    final_train_loss = train_losses[-1]\n",
    "    final_val_loss = val_losses[-1]\n",
    "    final_test_loss = test_losses[-1]\n",
    "    \n",
    "    print(f\"\\nModel training stopped after {len(train_losses)} epochs\")\n",
    "    if len(train_losses) < epochs:\n",
    "        print(f\"Early stopping activated (patience={early_stopping.patience})\")\n",
    "        \n",
    "    print(f\"Best epoch: {np.argmin(val_losses) + 1}\")\n",
    "    print(f\"Final training loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Final test loss: {final_test_loss:.4f}\")\n",
    "    print(f\"Best validation loss: {min(val_losses):.4f}\")\n",
    "    \n",
    "    return model, (train_losses, val_losses, test_losses)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df_full = pd.read_csv('Data/Clean/Orov_clean.csv')\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df_full['Date'] = pd.to_datetime(df_full['Date'])\n",
    "        \n",
    "        # Define date ranges for splits\n",
    "        test_start_date = pd.to_datetime('1923-01-01')\n",
    "        test_end_date = pd.to_datetime('1942-12-31')\n",
    "        train_start_date = pd.to_datetime('1963-01-01')\n",
    "        validation_start_date = pd.to_datetime('1943-01-01')\n",
    "        validation_end_date = pd.to_datetime('1962-12-31')\n",
    "        \n",
    "        # Create masks for each subset\n",
    "        test_mask = (df_full['Date'] >= test_start_date) & (df_full['Date'] <= test_end_date)\n",
    "        validation_mask = (df_full['Date'] >= validation_start_date) & (df_full['Date'] <= validation_end_date)\n",
    "        train_mask = df_full['Date'] >= train_start_date\n",
    "        \n",
    "        # Print dataset sizes\n",
    "        print(f\"Full dataset size: {len(df_full)}\")\n",
    "        print(f\"Training dataset size (1963-2021): {sum(train_mask)}\")\n",
    "        print(f\"Validation dataset size (1943-1962): {sum(validation_mask)}\")\n",
    "        print(f\"Test dataset size (1923-1942): {sum(test_mask)}\")\n",
    "        \n",
    "        #------------------------------------------------------------------------------\n",
    "        # Scenario 1: Basic preprocessing (just PCA with 4 components)\n",
    "        #------------------------------------------------------------------------------\n",
    "        print(\"\\n=== Scenario 1: Basic Preprocessing (PCA with 4 components) ===\")\n",
    "        \n",
    "        # Initialize and fit basic preprocessor on all data\n",
    "        basic_preprocessor = BasicDataPreprocessor()\n",
    "        basic_features = basic_preprocessor.fit_transform(df_full)\n",
    "        \n",
    "        # Save basic preprocessor\n",
    "        basic_preprocessor.save('basic_preprocessor.joblib')\n",
    "        \n",
    "        # Prepare sequences\n",
    "        basic_X_train, basic_y_train = prepare_sequences(\n",
    "            basic_features[train_mask], \n",
    "            df_full.loc[train_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        basic_X_val, basic_y_val = prepare_sequences(\n",
    "            basic_features[validation_mask], \n",
    "            df_full.loc[validation_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        basic_X_test, basic_y_test = prepare_sequences(\n",
    "            basic_features[test_mask], \n",
    "            df_full.loc[test_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        # Train basic model\n",
    "        basic_model, basic_losses = train_model(\n",
    "            basic_X_train, basic_y_train,\n",
    "            basic_X_val, basic_y_val,\n",
    "            basic_X_test, basic_y_test,\n",
    "            n_features=4,  # 4 PCs\n",
    "            approach=\"basic\"\n",
    "        )\n",
    "        \n",
    "        #------------------------------------------------------------------------------\n",
    "        # Scenario 2: Enhanced preprocessing (Detrending + Feature Engineering + PCA)\n",
    "        #------------------------------------------------------------------------------\n",
    "        print(\"\\n=== Scenario 2: Enhanced Preprocessing (Detrending + Feature Engineering + PCA) ===\")\n",
    "        \n",
    "        # Initialize and fit enhanced preprocessor on all data\n",
    "        enhanced_preprocessor = EnhancedDataPreprocessor()\n",
    "        enhanced_features = enhanced_preprocessor.fit_transform(df_full)\n",
    "        \n",
    "        # Save enhanced preprocessor\n",
    "        enhanced_preprocessor.save('enhanced_preprocessor.joblib')\n",
    "        \n",
    "        # Prepare sequences\n",
    "        enhanced_X_train, enhanced_y_train = prepare_sequences(\n",
    "            enhanced_features[train_mask], \n",
    "            df_full.loc[train_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        enhanced_X_val, enhanced_y_val = prepare_sequences(\n",
    "            enhanced_features[validation_mask], \n",
    "            df_full.loc[validation_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        enhanced_X_test, enhanced_y_test = prepare_sequences(\n",
    "            enhanced_features[test_mask], \n",
    "            df_full.loc[test_mask, 'F'].values\n",
    "        )\n",
    "        \n",
    "        # Train enhanced model\n",
    "        enhanced_model, enhanced_losses = train_model(\n",
    "            enhanced_X_train, enhanced_y_train,\n",
    "            enhanced_X_val, enhanced_y_val,\n",
    "            enhanced_X_test, enhanced_y_test,\n",
    "            n_features=5,  # 5 PCs\n",
    "            approach=\"enhanced\"\n",
    "        )\n",
    "        \n",
    "        #------------------------------------------------------------------------------\n",
    "        # Compare Results\n",
    "        #------------------------------------------------------------------------------\n",
    "        print(\"\\n=== Comparing Preprocessing Approaches ===\")\n",
    "        \n",
    "        # Plot comparison of training, validation, and test losses\n",
    "        comparison_plot = plot_comparison_losses(basic_losses, enhanced_losses, 'preprocessing_comparison_losses.png')\n",
    "        print(f\"Comparison plot saved to {comparison_plot}\")\n",
    "        \n",
    "        # Calculate and report improvement percentages\n",
    "        basic_best_val = min(basic_losses[1])\n",
    "        enhanced_best_val = min(enhanced_losses[1])\n",
    "        val_improvement = ((basic_best_val - enhanced_best_val) / basic_best_val) * 100\n",
    "        \n",
    "        basic_best_test = min(basic_losses[2])\n",
    "        enhanced_best_test = min(enhanced_losses[2])\n",
    "        test_improvement = ((basic_best_test - enhanced_best_test) / basic_best_test) * 100\n",
    "        \n",
    "        print(f\"\\nBest validation loss - Basic: {basic_best_val:.4f}, Enhanced: {enhanced_best_val:.4f}\")\n",
    "        print(f\"Validation improvement: {val_improvement:.2f}%\")\n",
    "        \n",
    "        print(f\"Best test loss - Basic: {basic_best_test:.4f}, Enhanced: {enhanced_best_test:.4f}\")\n",
    "        print(f\"Test improvement: {test_improvement:.2f}%\")\n",
    "        \n",
    "        #------------------------------------------------------------------------------\n",
    "        # Generate and Compare Predictions\n",
    "        #------------------------------------------------------------------------------\n",
    "        # Make predictions with both models\n",
    "        basic_predictions = basic_model.predict(basic_X_test)\n",
    "        enhanced_predictions = enhanced_model.predict(enhanced_X_test)\n",
    "        \n",
    "        # Plot sample predictions\n",
    "        n_samples = 3  # Number of samples to plot\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            plt.subplot(n_samples, 1, i+1)\n",
    "            plt.plot(basic_y_test[i], 'k-', linewidth=2, label='Actual')\n",
    "            plt.plot(basic_predictions[i], 'b--', linewidth=1.5, label='Basic Preprocessing')\n",
    "            plt.plot(enhanced_predictions[i], 'r--', linewidth=1.5, label='Enhanced Preprocessing')\n",
    "            plt.legend()\n",
    "            plt.title(f'Test Sample {i+1}')\n",
    "            plt.ylabel('Flow')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('preprocessing_comparison_predictions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nComparison predictions plot saved to preprocessing_comparison_predictions.png\")\n",
    "        print(\"\\nPreprocessing comparison analysis completed successfully!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DWR_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
